{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13467617-d8c3-4deb-a41a-1d1386f3967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf447a28-37ab-4c7c-8571-d2e47aa62025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_that_stuff(df):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(\"NACCUDSD\")\n",
    "    columns.append(\"NACCUDSD\")\n",
    "    return df.reindex(columns=columns)\n",
    "\n",
    "CSF = pd.read_csv(\"CSF dataset for pipeline - next steps are tt split, dropna in test, impute in train.csv\", index_col=\"Unnamed: 0\")\n",
    "MRI = pd.read_csv(\"MRI Dataset for pipeline - next steps are tt split.csv\", index_col=\"Unnamed: 0\")\n",
    "UDS = pd.read_csv(\"UDS dataset for pipeline - next steps are tt split.csv\", index_col=\"Unnamed: 0\")\n",
    "UDS_MRI = pd.read_csv(\"UDS_MRI dataset for pipeline - next steps are tt split.csv\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "UDS_MRI = reindex_that_stuff(UDS_MRI)\n",
    "CSF = reindex_that_stuff(CSF)\n",
    "MRI = reindex_that_stuff(MRI)\n",
    "UDS = reindex_that_stuff(UDS)\n",
    "\n",
    "# UDS_MRI = pd.read_csv(\"C:/Users/tminh/Downloads/results of yueqi code/DATA/Final 6.27 - \" + \"UDS_MRI_TRAIN\" + \".csv\", index_col=\"Unnamed: 0\").append(pd.read_csv(\"C:/Users/tminh/Downloads/results of yueqi code/DATA/Final 6.27 - \" + \"UDS_MRI_TEST\" + \".csv\", index_col=\"Unnamed: 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb6a006-417a-4f44-8703-1575fe46d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(df):\n",
    "  new_df = df.copy()\n",
    "  new_df['NACCUDSD'] = new_df['NACCUDSD'].replace(1, 0).replace([2, 3, 4], 1)\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e92e6f-a3fe-4a31-be58-152d67d92cc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0798db-e34d-4810-b43d-c4412e90db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSF Train set shape: (1251, 4)\n",
      "CSF Test set shape: (418, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      CSFABETA  CSFTTAU  CSFPTAU  NACCUDSD\n",
       " 0       447.00   424.00    58.30         1\n",
       " 1       922.45   263.11    59.52         1\n",
       " 2       162.65   906.65   106.15         4\n",
       " 3       755.00  1095.00   138.30         4\n",
       " 4      1023.00   205.00    35.30         1\n",
       " ...        ...      ...      ...       ...\n",
       " 1246    899.00   251.00    32.90         1\n",
       " 1247    618.00   534.00    74.90         3\n",
       " 1248    299.27    32.90    12.91         1\n",
       " 1249    779.00   518.00    68.50         1\n",
       " 1250    591.00   717.00   113.90         1\n",
       " \n",
       " [1251 rows x 4 columns],\n",
       " 'Mid-pipeline/CSF_TRAIN-42.csv',\n",
       "       CSFABETA  CSFTTAU  CSFPTAU  NACCUDSD\n",
       " 1770    739.00   171.00    23.50         1\n",
       " 1101    493.72    39.43    28.58         1\n",
       " 697     222.77    84.77    43.27         4\n",
       " 2731    680.00   176.00    32.00         1\n",
       " 2951   1075.00   331.55    64.52         1\n",
       " ...        ...      ...      ...       ...\n",
       " 2016    463.00   460.00    61.50         4\n",
       " 2776    395.00   195.00    22.00         1\n",
       " 631     162.00    30.00    16.00         4\n",
       " 91      169.50    36.10    39.90         4\n",
       " 229     337.77    68.74    19.94         1\n",
       " \n",
       " [377 rows x 4 columns],\n",
       " 'Mid-pipeline/CSF_TEST-42.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_csf(seed):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    shuffled_df = CSF.sample(frac=1, random_state=seed)\n",
    "    train_df, test_df = train_test_split(shuffled_df, test_size=0.25, random_state=seed)\n",
    "\n",
    "    print(\"CSF Train set shape:\", train_df.shape)\n",
    "    print(\"CSF Test set shape:\", test_df.shape)\n",
    "    \n",
    "    # SAVING\n",
    "    test_df_name = 'Mid-pipeline/CSF_TEST-' + str(seed) + '.csv'\n",
    "    test_df = test_df.dropna()\n",
    "    test_df.to_csv(test_df_name)\n",
    "    \n",
    "    from sklearn.impute import KNNImputer\n",
    "    \n",
    "    CSF_imputed = train_df[['CSFABETA', 'CSFTTAU', 'CSFPTAU']] # w/out NACCUDSD\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    CSF_imputed = imputer.fit_transform(CSF_imputed)\n",
    "\n",
    "    CSF_imputed = pd.DataFrame(CSF_imputed, columns = ['CSFABETA', 'CSFTTAU', 'CSFPTAU'])\n",
    "    CSF_imputed['NACCUDSD'] = train_df.reset_index()['NACCUDSD']\n",
    "    \n",
    "    # SAVING    \n",
    "    train_df_name = 'Mid-pipeline/CSF_TRAIN-' + str(seed) + '.csv'\n",
    "    CSF_imputed.to_csv(train_df_name)\n",
    "    \n",
    "    return CSF_imputed, train_df_name, test_df, test_df_name\n",
    "\n",
    "split_csf(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bbd2277-e7b0-4c7b-9e2a-6d353293b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (1239, 156)\n",
      "Test set shape: (414, 156)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(       NACCICV  NACCBRNV  NACCWMVL   CSFVOL  GRAYVOL  WHITEVOL   WMHVOL  \\\n",
       " 245   1309.939   979.232   412.224  324.267  573.448   405.784   6.4401   \n",
       " 1069  1348.544  1002.781   456.649  338.298  553.597   449.184   7.4650   \n",
       " 1564  1536.840  1217.530   534.820  319.310  682.710   534.820   0.0000   \n",
       " 368   1528.410  1073.670   460.020  439.870  628.520   445.150  14.8700   \n",
       " 627   1661.180  1174.312   552.988  473.883  634.309   540.003  12.9847   \n",
       " ...        ...       ...       ...      ...      ...       ...      ...   \n",
       " 296   1277.296   984.181   425.073  289.742  562.481   421.700   3.3732   \n",
       " 335   1349.506   972.928   446.479  325.322  577.705   395.223  51.2560   \n",
       " 1038  1672.960  1179.260   501.410  471.690  699.860   479.400  22.0100   \n",
       " 189   1170.211   897.902   416.863  264.556  488.792   409.110   7.7533   \n",
       " 374   1383.417   991.867   406.221  376.903  600.293   391.574  14.6470   \n",
       " \n",
       "       HIPPOVOL   CEREALL  CERETISS  ...  RSUPFRM  RSUPPAR  RSUPPARM  RSUPTEM  \\\n",
       " 245     6.5531  1117.120   851.070  ...   2.2828  10.3025    1.2630  12.9403   \n",
       " 1069    5.4270  1159.820   874.737  ...   1.9786   8.8850    1.1341  13.4180   \n",
       " 1564    7.9900  1295.000  1040.490  ...   2.2900  12.3000    1.7300  15.3700   \n",
       " 368     5.9200  1333.510   960.730  ...   2.7500  12.8700    1.6400  16.9800   \n",
       " 627     5.7836  1423.670  1018.240  ...   1.5596  10.8290    0.9669  14.5039   \n",
       " ...        ...       ...       ...  ...      ...      ...       ...      ...   \n",
       " 296     6.4244  1104.580   859.941  ...   1.8321  11.0736    1.2434  14.0062   \n",
       " 335     7.4243  1167.240   892.039  ...   1.4736  10.3496    1.8466  14.2163   \n",
       " 1038    7.6600  1443.600  1080.370  ...   4.0600  11.0300    2.2800  17.2100   \n",
       " 189     5.9165   946.881   746.694  ...   2.1086   8.0580    1.0856  11.7916   \n",
       " 374     6.6990  1195.810   883.546  ...   2.1985  11.5680    1.8245  12.8890   \n",
       " \n",
       "       RSUPTEMM  RSUPMAR  RSUPMARM  RTRTEM  RTRTEMM  NACCUDSD  \n",
       " 245     2.6150   8.6274    1.6028  0.7667   1.1421         1  \n",
       " 1069    1.9760   7.6430    1.8179  0.6000   1.0914         1  \n",
       " 1564    2.4900   9.7500    1.8900  1.2700   2.1700         1  \n",
       " 368     2.5800   6.7400    2.0800  0.8900   1.4700         1  \n",
       " 627     1.8488   9.8519    1.3869  0.5808   0.7856         3  \n",
       " ...        ...      ...       ...     ...      ...       ...  \n",
       " 296     2.0245   8.4672    1.5990  1.0028   1.1297         1  \n",
       " 335     2.3800   7.5888    1.7044  0.5708   0.6765         1  \n",
       " 1038    3.1100  11.1100    2.2000  1.0400   1.8800         1  \n",
       " 189     2.3088   6.4487    1.6251  0.5050   0.6726         1  \n",
       " 374     1.9829   6.9170    1.6632  0.8530   1.0839         1  \n",
       " \n",
       " [1239 rows x 156 columns],\n",
       " 'Mid-pipeline/MRI_TRAIN-32.csv',\n",
       "        NACCICV  NACCBRNV  NACCWMVL   CSFVOL  GRAYVOL  WHITEVOL   WMHVOL  \\\n",
       " 623   1276.130   965.467   432.456  306.041  537.633   427.834   4.6220   \n",
       " 1111  1511.040  1100.610   517.710  392.570  600.760   499.850  17.8600   \n",
       " 1177  1371.775   946.015   401.700  418.330  551.745   394.270   7.4300   \n",
       " 630   1502.560  1195.670   513.240  306.860  682.460   513.210   0.0300   \n",
       " 1010  1356.890   992.495   424.107  362.393  570.390   422.105   2.0020   \n",
       " ...        ...       ...       ...      ...      ...       ...      ...   \n",
       " 90    1528.400  1105.820   487.800  391.970  648.630   457.190  30.6100   \n",
       " 768   1368.130  1081.900   469.440  285.920  612.770   469.130   0.3100   \n",
       " 916   1632.359  1188.287   514.821  443.489  674.049   514.238   0.5830   \n",
       " 1439  1564.537  1159.067   511.221  390.360  662.956   496.111  15.1098   \n",
       " 1600  1476.290  1089.760   467.290  386.160  622.840   466.920   0.3700   \n",
       " \n",
       "       HIPPOVOL  CEREALL  CERETISS  ...  RSUPFRM  RSUPPAR  RSUPPARM  RSUPTEM  \\\n",
       " 623     5.5390  1111.81   853.595  ...   1.6875  11.3354    1.4473  12.6171   \n",
       " 1111    4.0100  1298.10   975.720  ...   2.1600  10.3000    1.3900  12.9900   \n",
       " 1177    6.2470  1199.41   847.067  ...   3.2395  10.3810    2.0005  11.8200   \n",
       " 630     6.9100  1276.21  1030.060  ...   2.2600  11.0200    1.4000  16.4200   \n",
       " 1010    6.8360  1143.86   849.390  ...   2.2380  10.3940    1.7031  14.1670   \n",
       " ...        ...      ...       ...  ...      ...      ...       ...      ...   \n",
       " 90      7.4100  1327.44   995.230  ...   2.4300  10.8200    1.7000  13.7700   \n",
       " 768     5.6700  1172.60   938.210  ...   2.8600  10.5300    1.5100  14.4200   \n",
       " 916     6.2690  1438.61  1053.050  ...   1.8379   9.7520    1.1872  15.3490   \n",
       " 1439    6.0399  1336.19  1008.960  ...   2.3923  12.5003    1.3196  15.0230   \n",
       " 1600    4.5700  1249.95   928.500  ...   2.0700   9.2900    1.6200  14.6000   \n",
       " \n",
       "       RSUPTEMM  RSUPMAR  RSUPMARM  RTRTEM  RTRTEMM  NACCUDSD  \n",
       " 623     2.3439   9.8963    1.9644  0.9713   1.4765         4  \n",
       " 1111    1.6000   9.6400    1.7800  0.7200   1.1500         4  \n",
       " 1177    2.0686   8.7170    2.1590  0.7400   1.2208         1  \n",
       " 630     2.6600  11.3300    1.9100  0.9900   2.0200         1  \n",
       " 1010    2.4387   9.2110    2.0212  1.0490   1.6878         1  \n",
       " ...        ...      ...       ...     ...      ...       ...  \n",
       " 90      2.1000  11.1200    2.1400  1.3100   1.5200         1  \n",
       " 768     2.2800   9.7800    2.1700  0.6900   1.9800         1  \n",
       " 916     2.0219  11.7810    1.9055  1.2870   1.8357         1  \n",
       " 1439    1.9919  10.4861    1.5944  1.0331   1.2296         2  \n",
       " 1600    2.1700   8.1400    1.8100  1.1600   1.8100         3  \n",
       " \n",
       " [414 rows x 156 columns],\n",
       " 'Mid-pipeline/MRI_TEST-32.csv')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_mri(seed):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    shuffled_df = MRI.sample(frac=1, random_state=seed)\n",
    "    train_df, test_df = train_test_split(shuffled_df, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # SAVING\n",
    "    test_df_name = 'Mid-pipeline/MRI_TEST-' + str(seed) + '.csv'\n",
    "    test_df.dropna().to_csv(test_df_name)\n",
    "    \n",
    "    # SAVING    \n",
    "    train_df_name = 'Mid-pipeline/MRI_TRAIN-' + str(seed) + '.csv'\n",
    "    train_df.to_csv(train_df_name)\n",
    "    \n",
    "    \n",
    "    print(\"Train set shape:\", train_df.shape)\n",
    "    print(\"Test set shape:\", test_df.shape)\n",
    "    \n",
    "    return train_df, train_df_name, test_df, test_df_name\n",
    "\n",
    "split_mri(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a1c9eff-a87c-4560-8dda-b3e4a61fb2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (14535, 110)\n",
      "Test set shape: (4846, 110)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(       NACCAPOE  ANIMALS   VEG  TRAILA  TRAILARR  TRAILALI  TRAILB  TRAILBRR  \\\n",
       " 719         1.0      8.0   4.0    45.0       0.0      24.0   996.0      96.0   \n",
       " 7395        2.0      5.0   1.0   997.0      97.0      97.0   997.0      97.0   \n",
       " 12882       2.0     17.0  12.0    26.0       0.0      24.0   163.0       2.0   \n",
       " 2768        3.0     23.0  18.0    35.0       0.0      24.0    86.0       0.0   \n",
       " 15245       1.0     17.0  11.0   996.0      96.0      96.0   996.0      96.0   \n",
       " ...         ...      ...   ...     ...       ...       ...     ...       ...   \n",
       " 11503       1.0     25.0  22.0    20.0       0.0      24.0    40.0       0.0   \n",
       " 12266       3.0     15.0   8.0    94.0       0.0      24.0   300.0       3.0   \n",
       " 3998        1.0     15.0   8.0    31.0       0.0      24.0    88.0       0.0   \n",
       " 11729       1.0     12.0   7.0    35.0       0.0      24.0    77.0       0.0   \n",
       " 8056        2.0     96.0  96.0   996.0      96.0      96.0   996.0      96.0   \n",
       " \n",
       "        TRAILBLI  DECSUB  ...  NACCMOM  NACCDAD  NACCFADM  NACCFFTD  INRELTO  \\\n",
       " 719        96.0       1  ...      0.0      0.0         0         0      1.0   \n",
       " 7395       97.0       1  ...      1.0      0.0         0         0      2.0   \n",
       " 12882      24.0       1  ...      0.0      0.0         0         0      1.0   \n",
       " 2768       24.0       1  ...      0.0      0.0         0         0      2.0   \n",
       " 15245      96.0       1  ...      0.0      0.0         0         0      1.0   \n",
       " ...         ...     ...  ...      ...      ...       ...       ...      ...   \n",
       " 11503      24.0       0  ...      9.0      9.0         0         0      1.0   \n",
       " 12266      14.0       0  ...      0.0      0.0         0         0      5.0   \n",
       " 3998       24.0       1  ...      0.0      0.0         0         0      1.0   \n",
       " 11729      24.0       1  ...      9.0      0.0         0         0      5.0   \n",
       " 8056       96.0       1  ...      0.0      0.0         0         0      1.0   \n",
       " \n",
       "        INLIVWTH  INVISITS  INCALLS  INRELY  NACCUDSD  \n",
       " 719         1.0       8.0      8.0     0.0         4  \n",
       " 7395        1.0       8.0      8.0     0.0         4  \n",
       " 12882       0.0       2.0      1.0     0.0         1  \n",
       " 2768        0.0       6.0      2.0     0.0         1  \n",
       " 15245       1.0       8.0      8.0     0.0         4  \n",
       " ...         ...       ...      ...     ...       ...  \n",
       " 11503       1.0       8.0      8.0     0.0         1  \n",
       " 12266       0.0       5.0      3.0     0.0         3  \n",
       " 3998        1.0       8.0      8.0     0.0         3  \n",
       " 11729       0.0       2.0      1.0     0.0         3  \n",
       " 8056        1.0       8.0      8.0     0.0         4  \n",
       " \n",
       " [14535 rows x 110 columns],\n",
       " 'Mid-pipeline/UDS_TRAIN-32.csv',\n",
       "        NACCAPOE  ANIMALS   VEG  TRAILA  TRAILARR  TRAILALI  TRAILB  TRAILBRR  \\\n",
       " 12558       1.0     16.0  12.0    28.0       0.0      24.0    90.0       3.0   \n",
       " 8191        3.0     18.0  14.0    30.0       0.0      24.0    67.0       0.0   \n",
       " 12309       1.0     22.0  15.0    32.0       0.0      24.0   108.0       1.0   \n",
       " 18725       1.0     18.0   9.0    32.0       0.0      24.0    76.0       0.0   \n",
       " 7197        2.0     20.0  11.0    34.0       0.0      24.0   111.0       1.0   \n",
       " ...         ...      ...   ...     ...       ...       ...     ...       ...   \n",
       " 5466        2.0     96.0  96.0   996.0      96.0      96.0   996.0      96.0   \n",
       " 2579        4.0      3.0   3.0    86.0       0.0      24.0   997.0      97.0   \n",
       " 10436       1.0      4.0   6.0   150.0       2.0      16.0   996.0      96.0   \n",
       " 1287        2.0      8.0   6.0    70.0       0.0      24.0   114.0       1.0   \n",
       " 12590       3.0     14.0  10.0   996.0      96.0      96.0   996.0      96.0   \n",
       " \n",
       "        TRAILBLI  DECSUB  ...  NACCMOM  NACCDAD  NACCFADM  NACCFFTD  INRELTO  \\\n",
       " 12558      24.0       0  ...      0.0      0.0         0         0      2.0   \n",
       " 8191       24.0       0  ...      0.0      0.0         0         0      5.0   \n",
       " 12309      24.0       0  ...      0.0      9.0         0         0      1.0   \n",
       " 18725      24.0       0  ...      0.0      1.0         0         0      1.0   \n",
       " 7197       24.0       1  ...      0.0      0.0         0         0      2.0   \n",
       " ...         ...     ...  ...      ...      ...       ...       ...      ...   \n",
       " 5466       96.0       1  ...      0.0      0.0         0         0      1.0   \n",
       " 2579       97.0       1  ...      0.0      0.0         0         0      2.0   \n",
       " 10436      96.0       0  ...      0.0      0.0         0         0      1.0   \n",
       " 1287       24.0       0  ...      0.0      0.0         0         0      2.0   \n",
       " 12590      96.0       1  ...      0.0      0.0         0         0      2.0   \n",
       " \n",
       "        INLIVWTH  INVISITS  INCALLS  INRELY  NACCUDSD  \n",
       " 12558       0.0       2.0      1.0     0.0         1  \n",
       " 8191        0.0       4.0      6.0     0.0         1  \n",
       " 12309       1.0       8.0      8.0     0.0         1  \n",
       " 18725       1.0       8.0      8.0     0.0         1  \n",
       " 7197        0.0       3.0      3.0     0.0         1  \n",
       " ...         ...       ...      ...     ...       ...  \n",
       " 5466        1.0       8.0      8.0     0.0         4  \n",
       " 2579        0.0       1.0      1.0     0.0         4  \n",
       " 10436       1.0       8.0      8.0     0.0         4  \n",
       " 1287        1.0       8.0      8.0     0.0         1  \n",
       " 12590       0.0       3.0      2.0     0.0         1  \n",
       " \n",
       " [4846 rows x 110 columns],\n",
       " 'Mid-pipeline/UDS_TEST-32.csv')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_uds(seed):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    shuffled_df = UDS.sample(frac=1, random_state=seed)\n",
    "    train_df, test_df = train_test_split(shuffled_df, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # SAVING\n",
    "    test_df_name = 'Mid-pipeline/UDS_TEST-' + str(seed) + '.csv'\n",
    "    test_df.dropna().to_csv(test_df_name)\n",
    "    \n",
    "    # SAVING    \n",
    "    train_df_name = 'Mid-pipeline/UDS_TRAIN-' + str(seed) + '.csv'\n",
    "    train_df.to_csv(train_df_name)\n",
    "    \n",
    "    \n",
    "    print(\"Train set shape:\", train_df.shape)\n",
    "    print(\"Test set shape:\", test_df.shape)\n",
    "    \n",
    "    return train_df, train_df_name, test_df, test_df_name\n",
    "\n",
    "split_uds(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0199cb3-da6c-4bd5-ba3a-faae054ed1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (1068, 265)\n",
      "Test set shape: (357, 265)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      NACCAPOE  ANIMALS   VEG  TRAILA  TRAILARR  TRAILALI  TRAILB  TRAILBRR  \\\n",
       " 66         2.0      9.0   3.0    32.0       1.0      24.0    91.0       1.0   \n",
       " 527        2.0     19.0  18.0    30.0       0.0      24.0    86.0       1.0   \n",
       " 1235       1.0     22.0  15.0    17.0       0.0      24.0    34.0       0.0   \n",
       " 1098       1.0      5.0  10.0   110.0       0.0      24.0   300.0       2.0   \n",
       " 188        1.0     20.0  13.0    28.0       0.0      24.0    69.0       1.0   \n",
       " ...        ...      ...   ...     ...       ...       ...     ...       ...   \n",
       " 1154       2.0     31.0  23.0    16.0       0.0      24.0    35.0       0.0   \n",
       " 157        3.0     21.0  22.0    18.0       0.0      24.0    46.0       0.0   \n",
       " 1124       2.0     26.0  12.0    27.0       1.0      24.0    48.0       0.0   \n",
       " 125        1.0     13.0  18.0    48.0       1.0      24.0   128.0       0.0   \n",
       " 877        2.0     23.0  15.0    26.0       0.0      24.0   121.0       2.0   \n",
       " \n",
       "       TRAILBLI  DECSUB  ...  RSUPFRM  RSUPPAR  RSUPPARM  RSUPTEM  RSUPTEMM  \\\n",
       " 66        24.0       1  ...   2.1295  12.0148    1.8415  13.7515    2.0263   \n",
       " 527       24.0       1  ...   2.2900  10.7800    1.6800  16.4200    2.6900   \n",
       " 1235      24.0       0  ...   2.5100   9.5100    1.6000  12.4900    2.2700   \n",
       " 1098      17.0       1  ...   2.1800   7.9100    1.1400   9.5200    1.4400   \n",
       " 188       24.0       0  ...   2.3200  11.4290    1.5012  14.1950    2.2514   \n",
       " ...        ...     ...  ...      ...      ...       ...      ...       ...   \n",
       " 1154      24.0       0  ...   2.6300  14.0700    1.3400  16.5300    2.2800   \n",
       " 157       24.0       0  ...   2.6400  11.9000    1.9400  13.7300    2.1700   \n",
       " 1124      24.0       0  ...   2.4007  13.2730    1.6073  18.9760    1.9308   \n",
       " 125       24.0       0  ...   2.8923   9.0330    1.6190  11.3110    1.6890   \n",
       " 877       24.0       0  ...   2.5132   9.4212    1.4931  12.8258    2.6371   \n",
       " \n",
       "       RSUPMAR  RSUPMARM  RTRTEM  RTRTEMM  NACCUDSD  \n",
       " 66     9.7146    1.4304  0.8669   0.9496         3  \n",
       " 527   10.3600    1.9100  1.2600   1.7700         1  \n",
       " 1235   8.1100    1.8500  0.8600   1.9400         1  \n",
       " 1098   7.8000    1.7400  0.8000   1.5200         4  \n",
       " 188    8.2190    2.0881  0.8270   1.5756         1  \n",
       " ...       ...       ...     ...      ...       ...  \n",
       " 1154   9.3000    1.5600  1.4300   2.2900         1  \n",
       " 157    8.2300    2.1900  1.1600   1.7600         1  \n",
       " 1124  11.2460    1.7205  1.0570   1.7967         1  \n",
       " 125    7.7150    1.7154  0.8870   1.7375         1  \n",
       " 877    9.2024    1.9120  0.9227   1.6063         1  \n",
       " \n",
       " [1068 rows x 265 columns],\n",
       " 'Mid-pipeline/UDS_MRI_TRAIN-32.csv',\n",
       "       NACCAPOE  ANIMALS   VEG  TRAILA  TRAILARR  TRAILALI  TRAILB  TRAILBRR  \\\n",
       " 757        1.0     11.0   8.0    54.0       0.0      24.0    85.0       1.0   \n",
       " 1031       1.0     11.0   4.0   115.0       0.0      24.0   300.0       1.0   \n",
       " 461        1.0     25.0  19.0    22.0       0.0      24.0    79.0       0.0   \n",
       " 397        1.0     39.0  24.0    20.0       0.0      24.0    52.0       0.0   \n",
       " 683        2.0     26.0  18.0    15.0       0.0      24.0    41.0       0.0   \n",
       " ...        ...      ...   ...     ...       ...       ...     ...       ...   \n",
       " 767        2.0     21.0  11.0    32.0       0.0      24.0    53.0       0.0   \n",
       " 261        1.0     15.0  12.0    84.0       0.0      24.0   300.0       3.0   \n",
       " 704        2.0     11.0   6.0    76.0       0.0      24.0   244.0       1.0   \n",
       " 21         1.0     23.0  18.0    38.0       0.0      24.0    79.0       0.0   \n",
       " 1150       2.0     17.0   5.0    58.0       0.0      24.0   132.0       0.0   \n",
       " \n",
       "       TRAILBLI  DECSUB  ...  RSUPFRM  RSUPPAR  RSUPPARM  RSUPTEM  RSUPTEMM  \\\n",
       " 757       24.0       1  ...   2.4235  11.3083    1.7907  14.2165    2.5022   \n",
       " 1031      19.0       1  ...   2.3002   8.3959    1.4127  10.9576    1.7411   \n",
       " 461       24.0       0  ...   2.6400  10.6100    1.9700  13.4400    2.2900   \n",
       " 397       24.0       0  ...   1.9500  11.0800    1.2200  14.9800    2.5500   \n",
       " 683       24.0       1  ...   2.1616   9.4030    1.6394  12.8220    1.9254   \n",
       " ...        ...     ...  ...      ...      ...       ...      ...       ...   \n",
       " 767       24.0       0  ...   2.5029  11.9140    1.4957  14.2980    2.0073   \n",
       " 261       16.0       0  ...   1.8321  11.0736    1.2434  14.0062    2.0245   \n",
       " 704       24.0       1  ...   2.0562   7.6375    1.2434  10.5629    1.8836   \n",
       " 21        24.0       1  ...   2.5175   9.6600    1.6361  13.3250    2.3901   \n",
       " 1150      24.0       1  ...   2.2093  11.0834    1.4266  13.6884    2.6665   \n",
       " \n",
       "       RSUPMAR  RSUPMARM  RTRTEM  RTRTEMM  NACCUDSD  \n",
       " 757    8.6618    1.9829  0.6938   1.2310         4  \n",
       " 1031   9.5993    2.0335  0.6116   1.0273         4  \n",
       " 461    8.8800    1.9700  1.0500   1.6900         1  \n",
       " 397    8.3000    1.7400  0.8400   2.0300         1  \n",
       " 683   10.1790    1.6944  0.7700   1.6293         1  \n",
       " ...       ...       ...     ...      ...       ...  \n",
       " 767    9.3200    1.8611  1.1920   1.7712         1  \n",
       " 261    8.4672    1.5990  1.0028   1.1297         1  \n",
       " 704    7.5588    1.6807  0.6566   0.9930         4  \n",
       " 21     7.8670    1.6894  0.8080   1.3027         1  \n",
       " 1150  10.7959    1.7124  0.6809   1.0637         3  \n",
       " \n",
       " [357 rows x 265 columns],\n",
       " 'Mid-pipeline/UDS_MRI_TEST-32.csv')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_uds_mri(seed):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    shuffled_df = UDS_MRI.sample(frac=1, random_state=seed)\n",
    "    train_df, test_df = train_test_split(shuffled_df, test_size=0.25, random_state=seed)\n",
    "\n",
    "    # SAVING\n",
    "    test_df_name = 'Mid-pipeline/UDS_MRI_TEST-' + str(seed) + '.csv'\n",
    "    test_df.to_csv(test_df_name)\n",
    "    \n",
    "    # SAVING    \n",
    "    train_df_name = 'Mid-pipeline/UDS_MRI_TRAIN-' + str(seed) + '.csv'\n",
    "    train_df.to_csv(train_df_name)\n",
    "    \n",
    "    \n",
    "    print(\"Train set shape:\", train_df.shape)\n",
    "    print(\"Test set shape:\", test_df.shape)\n",
    "    \n",
    "    return train_df, train_df_name, test_df, test_df_name\n",
    "\n",
    "split_uds_mri(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05bf4a-f255-49ec-99bf-f93d9a55db8f",
   "metadata": {
    "id": "ncE6BEGCkbYw",
    "tags": []
   },
   "source": [
    "# Fusion Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285900-94e4-440b-b4b4-a77f69760270",
   "metadata": {
    "id": "dvS0MhKP9oaI",
    "tags": []
   },
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a763a2f2-ae1d-46d7-b014-12ac3428e341",
   "metadata": {
    "id": "cNiy5ja828ii"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import gc\n",
    "\n",
    "def AE(df, epochs = 10):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']  # Response variable\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0., random_state=42)\n",
    "  y_train = y_categorical\n",
    "\n",
    "  # Scale the input features\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X)\n",
    "  # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  # Create the neural network model\n",
    "  input_shape=(X_train_scaled.shape[1],)\n",
    "  print(input_shape)\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation='relu', input_shape = input_shape))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(4, activation='relu'))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(input_shape[0], activation='softmax'))  # Output layer with softmax activation for categorical variables\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train_scaled, X_train_scaled, epochs=epochs, batch_size=16)\n",
    "\n",
    "  return model, scaler\n",
    "# PART 2: GET VALUES\n",
    "\n",
    "def through_AE(df, model, scaler):\n",
    "  print(\"started\")\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # Scale the input features\n",
    "  # scaler = StandardScaler()\n",
    "  X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "  import tensorflow as tf\n",
    "  # Create a new model to get values from the last dense layer before output\n",
    "  last_dense_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-4].output)\n",
    "\n",
    "  output_values = last_dense_layer_model.predict(X_scaled)\n",
    "  new_df = pd.DataFrame(output_values)\n",
    "  new_df['NACCUDSD'] = df.reset_index()['NACCUDSD']\n",
    "\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba4ebb-3ff0-4af8-b6b2-1f6b1e98167e",
   "metadata": {
    "id": "WKr_8HVLfQOW",
    "tags": []
   },
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aaa23ac-8404-48cf-a89f-ed0f81388372",
   "metadata": {
    "id": "le97dzvMci28"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def lasso(df, al, thresh):\n",
    "\n",
    "  # Create the Lasso model\n",
    "  lasso = Lasso(alpha=al)\n",
    "\n",
    "  # Select the features\n",
    "  selector = SelectFromModel(lasso, threshold=thresh)\n",
    "  selector.fit(df.drop(\"NACCUDSD\", axis=1), df[\"NACCUDSD\"])\n",
    "\n",
    "  # Get the selected features\n",
    "  selected_features = selector.get_support()\n",
    "\n",
    "  return df.columns[:-1][selected_features].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c3ff6-6fab-46cc-b3c1-7f2f901897d2",
   "metadata": {
    "id": "hDVacQ-7g4L5",
    "tags": []
   },
   "source": [
    "## SDNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92bfbda-5a6f-4341-9334-c1252a2cef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def through_SDNN_old(df, model, scaler):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Scale the input features\n",
    "  # scaler = StandardScaler()\n",
    "  X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "  import tensorflow as tf\n",
    "  # Create a new model to get values from the last dense layer before output\n",
    "  last_dense_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "\n",
    "    \n",
    "  pickle = []\n",
    "  for i in range(0, len(X_scaled)):\n",
    "    # print(i)\n",
    "    if i % 1000 == 0:\n",
    "      print(str(i) + \" / \" + str(len(X_scaled)))\n",
    "      that_df = pd.DataFrame(pickle)\n",
    "\n",
    "    input_row = X_scaled[i]  # Replace with your desired input row\n",
    "\n",
    "    # Obtain values from the last dense layer before output for the input row\n",
    "    output_values = last_dense_layer_model.predict(input_row.reshape(1, -1), verbose=0)[0]\n",
    "    # print(output_values)\n",
    "    pickle.append(output_values)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "  # print('Output values:', output_values)\n",
    "  # new_df = pd.DataFrame(output_values)\n",
    "  new_df = pd.DataFrame(pickle)\n",
    "  new_df['NACCUDSD'] = df.reset_index()['NACCUDSD']\n",
    "\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85577aee-8af8-4cd9-9b34-959c21b4fdf8",
   "metadata": {
    "id": "__EHBhUZg5ah"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import gc\n",
    "\n",
    "def SDNN(df, epochs = 10):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']  # Response variable\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0., random_state=42)\n",
    "  y_train = y_categorical\n",
    "\n",
    "  # Scale the input features\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X)\n",
    "  # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  # Create the neural network model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(10, activation='relu'))\n",
    "  model.add(Dense(y_categorical.shape[1], activation='softmax'))  # Output layer with softmax activation for categorical variables\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=16)\n",
    "\n",
    "  return model, scaler\n",
    "# PART 2: GET VALUES\n",
    "\n",
    "def through_SDNN(df, model, scaler):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Scale the input features\n",
    "  # scaler = StandardScaler()\n",
    "  X_scaled = scaler.transform(X)\n",
    "\n",
    "    \n",
    "  import tensorflow as tf\n",
    "  # Create a new model to get values from the last dense layer before output\n",
    "  last_dense_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "\n",
    "  output_values = last_dense_layer_model.predict(X_scaled, verbose=0)\n",
    "\n",
    "  # print('Output values:', output_values)\n",
    "  new_df = pd.DataFrame(output_values)\n",
    "  # new_df = pd.DataFrame(pickle)\n",
    "  new_df['NACCUDSD'] = df.reset_index()['NACCUDSD']\n",
    "\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c368eb-8169-48bd-8261-d13d8b65fad0",
   "metadata": {
    "id": "30byG0SHRYSZ",
    "tags": []
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b89712e-610f-447f-8723-470697a4a6c5",
   "metadata": {
    "id": "yLu1TCSwRZv2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca(train_df, test_df, var = 0.5):\n",
    "\n",
    "\n",
    "  # Load and preprocess the training dataset\n",
    "  train_data = train_df.reset_index().copy()\n",
    "  train_features = train_data.drop('NACCUDSD', axis=1)  # Exclude the response variable\n",
    "\n",
    "  # Standardize the training features by removing the mean and scaling to unit variance\n",
    "  scaler = StandardScaler()\n",
    "  scaled_train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "  # Perform PCA on the training features\n",
    "  pca = PCA()\n",
    "  pca.fit(scaled_train_features)\n",
    "\n",
    "  # Load and preprocess the test dataset\n",
    "  test_data = test_df.reset_index().copy()\n",
    "  test_features = test_data.drop('NACCUDSD', axis=1)  # Exclude the response variable\n",
    "\n",
    "  # Standardize the test features using the same scaler applied to the training features\n",
    "  scaled_test_features = scaler.transform(test_features)\n",
    "\n",
    "  # Apply the learned PCA model to both training and test features\n",
    "  train_pca = pca.transform(scaled_train_features)\n",
    "  test_pca = pca.transform(scaled_test_features)\n",
    "\n",
    "  # Construct new DataFrames for the transformed features\n",
    "  train_pca_df = pd.DataFrame(data=train_pca, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "  test_pca_df = pd.DataFrame(data=test_pca, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "\n",
    "  # Concatenate the response variable with the PCA-transformed training features\n",
    "  train_pca_df = pd.concat([train_pca_df, train_data['NACCUDSD']], axis=1)\n",
    "  test_pca_df = pd.concat([test_pca_df, test_data['NACCUDSD']], axis=1)\n",
    "\n",
    "  # Print the resulting PCA-transformed datasets\n",
    "  return [train_pca_df, test_pca_df]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4076c45-a626-4531-9c58-255b340a1dd7",
   "metadata": {
    "id": "TM_Nm3x1Aann"
   },
   "source": [
    "# Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a32b29-9216-40e9-8b7e-9614d2d9d887",
   "metadata": {
    "id": "hJG-CYRMAZEe"
   },
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a1774a-bd26-4496-bfa4-8132131c44ea",
   "metadata": {
    "id": "0dO-mR07Ake1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def logistic_regression(train_df, test_df, cols, give_table = False):\n",
    "\n",
    "  train_df = binarize(train_df)\n",
    "  test_df =  binarize(test_df)\n",
    "\n",
    "\n",
    "  X_train = train_df[cols]  # Input features # ERROR: y is included\n",
    "  y_train = train_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  X_test = test_df[cols]  # Input features # ERROR: y is included\n",
    "  y_test = test_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  logreg = LogisticRegression(solver='lbfgs')\n",
    "  logreg.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = logreg.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  print(accuracy)\n",
    "\n",
    "  # if give_table:\n",
    "  #       return y_pred\n",
    "\n",
    "  return [accuracy, y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b09184-249b-4767-9e1a-64e589f5c550",
   "metadata": {
    "id": "K4qAIgXvfRMc"
   },
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a23a848-0d0b-4e2e-b99e-60167e3a4ad1",
   "metadata": {
    "id": "DqYZfJVTfSha"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def random_forest(train_df, test_df, cols, give_table = False):\n",
    "  # Load the dataset\n",
    "  # dataset = UDS_MRI_CSF[concat_cols+['NACCUDSD']]\n",
    "  X_train = train_df[cols]  # Input features # ERROR: y is included\n",
    "  y_train = train_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  X_test = test_df[cols]  # Input features # ERROR: y is included\n",
    "  y_test = test_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "  rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "  # Fit the model to the training data\n",
    "  rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "  # Evaluate model performance\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  # print(\"Accuracy:\", accuracy)\n",
    "  print(accuracy)\n",
    "\n",
    "  # if give_table:\n",
    "  #       return y_pred\n",
    "\n",
    "  # return rf_classifier\n",
    "  return [accuracy, y_pred]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e114b-40e4-49fd-a660-8ed9622c2d38",
   "metadata": {
    "id": "I9k7STA-O-eX"
   },
   "source": [
    "## Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5a7644f-5930-4000-b463-7e85e365bbdb",
   "metadata": {
    "id": "YOovsfkRPEMd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Assuming you have train_df and test_df DataFrames\n",
    "# and the response variable is 'NACCUDSD'\n",
    "\n",
    "def neural_network(train_df, test_df, cols, epochs = 10, give_table = False):\n",
    "  # Separate features and target variable\n",
    "  X_train = train_df[cols]\n",
    "  y_train = train_df['NACCUDSD']-1\n",
    "  X_test = test_df[cols]\n",
    "  y_test = test_df['NACCUDSD']-1\n",
    "\n",
    "  # Convert the target variable to categorical\n",
    "  num_classes = len(set(y_train))\n",
    "  y_train_cat = to_categorical(y_train, num_classes)\n",
    "  y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "  # Standardize the features\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  # Create the model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train_scaled, y_train_cat, epochs=epochs, batch_size=32, verbose=1)\n",
    "\n",
    "  # Evaluate the model\n",
    "  loss, accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "\n",
    "  print(\"Test loss:\", loss)\n",
    "  print(\"Test accuracy:\", accuracy)\n",
    "    \n",
    "  if give_table:\n",
    "        preds = model.predict(X_test_scaled)\n",
    "        return np.argmax(preds, axis=1)\n",
    "\n",
    "  # return model\n",
    "  preds = model.predict(X_test_scaled)\n",
    "  \n",
    "  return [accuracy, np.argmax(preds, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4888a8c-8cbb-4d28-b661-390eeecc828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mri_feats = ['NACCICV', 'NACCBRNV',\n",
    "       'NACCWMVL', 'CSFVOL', 'GRAYVOL', 'WHITEVOL', 'WMHVOL', 'HIPPOVOL',\n",
    "       'CEREALL', 'CERETISS', 'CERECSF', 'CEREGR', 'CEREWH', 'LHIPPO',\n",
    "       'RHIPPO', 'LLATVENT', 'RLATVENT', 'LATVENT', 'THIRVENT', 'LFRCORT',\n",
    "       'RFRCORT', 'FRCORT', 'LOCCORT', 'ROCCORT', 'OCCCORT', 'LPARCORT',\n",
    "       'RPARCORT', 'PARCORT', 'LTEMPCOR', 'RTEMPCOR', 'TEMPCOR', 'LCAC',\n",
    "       'LCACM', 'LCMF', 'LCMFM', 'LCUN', 'LCUNM', 'LENT', 'LENTM', 'LFUS',\n",
    "       'LFUSM', 'LINFPAR', 'LINFPARM', 'LINFTEMP', 'LINFTEMM', 'LINSULA',\n",
    "       'LINSULAM', 'LISTHC', 'LISTHCM', 'LLATOCC', 'LLATOCCM', 'LLATORBF',\n",
    "       'LLATORBM', 'LLING', 'LLINGM', 'LMEDORBF', 'LMEDORBM', 'LMIDTEMP',\n",
    "       'LMIDTEMM', 'LPARCEN', 'LPARCENM', 'LPARHIP', 'LPARHIPM',\n",
    "       'LPARSOP', 'LPARSOPM', 'LPARORB', 'LPARORBM', 'LPARTRI',\n",
    "       'LPARTRIM', 'LPERCAL', 'LPERCALM', 'LPOSCEN', 'LPOSCENM',\n",
    "       'LPOSCIN', 'LPOSCINM', 'LPRECEN', 'LPRECENM', 'LPRECUN',\n",
    "       'LPRECUNM', 'LROSANC', 'LROSANCM', 'LROSMF', 'LROSMFM', 'LSUPFR',\n",
    "       'LSUPFRM', 'LSUPPAR', 'LSUPPARM', 'LSUPTEM', 'LSUPTEMM', 'LSUPMAR',\n",
    "       'LSUPMARM', 'LTRTEM', 'LTRTEMM', 'RCAC', 'RCACM', 'RCMF', 'RCMFM',\n",
    "       'RCUN', 'RCUNM', 'RENT', 'RENTM', 'RFUS', 'RFUSM', 'RINFPAR',\n",
    "       'RINFPARM', 'RINFTEMP', 'RINFTEMM', 'RINSULA', 'RINSULAM',\n",
    "       'RISTHC', 'RISTHCM', 'RLATOCC', 'RLATOCCM', 'RLATORBF', 'RLATORBM',\n",
    "       'RLING', 'RLINGM', 'RMEDORBF', 'RMEDORBM', 'RMIDTEMP', 'RMIDTEMM',\n",
    "       'RPARCEN', 'RPARCENM', 'RPARHIP', 'RPARHIPM', 'RPARSOP',\n",
    "       'RPARSOPM', 'RPARORB', 'RPARORBM', 'RPARTRI', 'RPARTRIM',\n",
    "       'RPERCAL', 'RPERCALM', 'RPOSCEN', 'RPOSCENM', 'RPOSCIN',\n",
    "       'RPOSCINM', 'RPRECEN', 'RPRECENM', 'RPRECUN', 'RPRECUNM',\n",
    "       'RROSANC', 'RROSANCM', 'RROSMF', 'RROSMFM', 'RSUPFR', 'RSUPFRM',\n",
    "       'RSUPPAR', 'RSUPPARM', 'RSUPTEM', 'RSUPTEMM', 'RSUPMAR',\n",
    "       'RSUPMARM', 'RTRTEM', 'RTRTEMM']\n",
    "\n",
    "input_demo_feats = ['SEX', 'HISPANIC', 'HISPOR', 'RACE', 'RACEX',\n",
    "       'PRIMLANG', 'EDUC', 'MARISTAT', 'NACCLIVS', 'INDEPEND', 'RESIDENC',\n",
    "       'NACCNIHR','NACCAGE']\n",
    "input_copart_feats = ['INEDUC', 'INRELTO', 'INKNOWN', 'INLIVWTH', 'INVISITS', 'INCALLS',\n",
    "       'INRELY']\n",
    "input_fam_hist_feats = ['NACCFAM', 'NACCMOM', 'NACCDAD', 'NACCAM', 'NACCAMX',\n",
    "       'NACCAMS', 'NACCAMSX', 'NACCFM', 'NACCFMX', 'NACCFMS', 'NACCFMSX',\n",
    "       'NACCOM', 'NACCOMX', 'NACCOMS', 'NACCOMSX', 'NACCFADM', 'NACCFFTD']\n",
    "input_patient_hist_feats = ['ANYMEDS', 'TOBAC30', 'TOBAC100', 'SMOKYRS', 'PACKSPER',\n",
    "       'QUITSMOK', 'CVHATT', 'CVAFIB', 'CVANGIO', 'CVBYPASS', 'CVPACDEF',\n",
    "       'CVPACE', 'CVCHF', 'CVOTHR', 'CBSTROKE', 'NACCSTYR', 'CBTIA',\n",
    "       'NACCTIYR', 'SEIZURES', 'NACCTBI', 'TBI', 'TBIBRIEF', 'TRAUMBRF',\n",
    "       'TBIEXTEN', 'TRAUMEXT', 'TBIWOLOS', 'TRAUMCHR', 'TBIYEAR',\n",
    "       'NCOTHR', 'DIABETES', 'DIABTYPE', 'HYPERTEN', 'HYPERCHO', 'B12DEF',\n",
    "       'THYROID', 'ARTHRIT', 'ARTHTYPE', 'ARTHTYPX', 'ARTHUPEX',\n",
    "       'ARTHLOEX', 'ARTHSPIN', 'ARTHUNK', 'INCONTU', 'INCONTF', 'APNEA',\n",
    "       'RBD', 'INSOMN', 'OTHSLEEP', 'OTHSLEEX', 'ALCOHOL', 'ABUSOTHR',\n",
    "       'ABUSX', 'PTSD', 'BIPOLAR', 'SCHIZ', 'DEP2YRS', 'DEPOTHR',\n",
    "       'ANXIETY', 'OCD', 'NPSYDEV', 'PSYCDIS', 'PSYCDISX',\n",
    "       'NACCAAAS', 'NACCAANX', 'NACCAC', 'NACCACEI',\n",
    "       'NACCADEP', 'NACCAHTN', 'NACCAMD', 'NACCANGI', 'NACCAPSY',\n",
    "       'NACCBETA', 'NACCCCBS', 'NACCDBMD', 'NACCDIUR', 'NACCEMD',\n",
    "       'NACCEPMD', 'NACCHTNC', 'NACCLIPL', 'NACCNSD', 'NACCPDMD',\n",
    "       'NACCVASD']\n",
    "input_physical_feats = ['HEIGHT','WEIGHT', 'BPSYS', 'BPDIAS', 'HRATE',\n",
    "        'VISION', 'VISCORR','VISWCORR', 'HEARING', 'HEARAID', 'HEARWAID',\n",
    "        'NACCBMI']\n",
    "input_exam_feats = ['FOCLSYM','FOCLSIGN','NACCNREX', 'NORMEXAM', 'DECSUB']\n",
    "input_npi_feats = ['NPIQINF', 'NPIQINFX', 'DEL', 'DELSEV', 'HALL',\n",
    "       'HALLSEV', 'AGIT', 'AGITSEV', 'DEPD', 'DEPDSEV', 'ANX', 'ANXSEV',\n",
    "       'ELAT', 'ELATSEV', 'APA', 'APASEV', 'DISN', 'DISNSEV', 'IRR',\n",
    "       'IRRSEV', 'MOT', 'MOTSEV', 'NITE', 'NITESEV', 'APP', 'APPSEV']\n",
    "input_gds_feats = ['NOGDS', 'SATIS', 'DROPACT', 'EMPTY', 'BORED', 'SPIRITS', 'AFRAID',\n",
    "       'HAPPY', 'HELPLESS', 'STAYHOME', 'MEMPROB', 'WONDRFUL', 'WRTHLESS',\n",
    "       'ENERGY', 'HOPELESS', 'BETTER', 'NACCGDS']\n",
    "input_faq_feats = ['BILLS', 'TAXES','SHOPPING', 'GAMES', 'STOVE',\n",
    "        'MEALPREP', 'EVENTS', 'PAYATTN','REMDATES', 'TRAVEL']\n",
    "input_scd_feats = ['DECSUB']\n",
    "intput_np_feats = ['MMSEORDA',\n",
    "       'MMSEORLO', 'PENTAGON', 'NACCMMSE', 'LOGIMEM', 'MEMUNITS',\n",
    "       'MEMTIME', 'DIGIF', 'DIGIFLEN', 'DIGIB', 'DIGIBLEN', 'ANIMALS',\n",
    "       'VEG', 'TRAILA', 'TRAILARR', 'TRAILALI', 'TRAILB', 'TRAILBRR',\n",
    "       'TRAILBLI', 'BOSTON', 'MOCATOTS']\n",
    "input_gene_feats = ['NACCAPOE']\n",
    "label_feat = ['NACCUDSD']\n",
    "# cdr_feats = ['MEMORY', 'ORIENT', 'JUDGMENT', 'COMMUN', 'HOMEHOBB', 'PERSCARE', 'CDRSUM', 'CDRGLOB']\n",
    "UDS_selected_features = label_feat + input_gene_feats + intput_np_feats + input_scd_feats + input_faq_feats + input_gds_feats + input_npi_feats + input_exam_feats + input_physical_feats + input_patient_hist_feats + input_fam_hist_feats + input_copart_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56253315-17fd-43cc-a4fe-54c34f98ccb3",
   "metadata": {},
   "source": [
    "# Helper functions - runs feature selection algs in one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5331afe-c11c-4285-803d-db4561e66399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_AE(train_df, test_df, e):\n",
    "    model, scaler = AE(train_df, epochs = e)\n",
    "    processed_AE_train = through_AE(train_df, model, scaler)\n",
    "    processed_AE_test = through_AE(test_df, model, scaler)\n",
    "    \n",
    "    return processed_AE_train, processed_AE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e56101-a2f4-4f6b-85f0-976eb68959c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_SE(train_df, test_df, e):\n",
    "    model, scaler = AE(train_df, epochs = e)\n",
    "    processed_AE_train = through_AE(train_df, model, scaler)\n",
    "    processed_AE_test = through_AE(test_df, model, scaler)\n",
    "    \n",
    "    return processed_AE_train, processed_AE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67e8c8cf-f091-424c-b628-7e40ce5fc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (1239, 156)\n",
      "Test set shape: (414, 156)\n",
      "(155,)\n",
      "Epoch 1/10\n",
      "78/78 [==============================] - 1s 2ms/step - loss: 0.9979 - accuracy: 0.0048\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9926 - accuracy: 0.0169\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9904 - accuracy: 0.0315\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9896 - accuracy: 0.0291\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9889 - accuracy: 0.0404\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9879 - accuracy: 0.0395\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9874 - accuracy: 0.0533\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9869 - accuracy: 0.0581\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9864 - accuracy: 0.0670\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9860 - accuracy: 0.0646\n",
      "started\n",
      "39/39 [==============================] - 0s 1ms/step\n",
      "started\n",
      "13/13 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "MRI_TRAIN, MRI_TRAIN_NAME, MRI_TEST, MRI_TEST_NAME = split_mri(53)\n",
    "CURR_TRAIN, CURR_TEST = helper_AE(MRI_TRAIN, MRI_TEST, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4000a06-a7a1-4b0d-b075-87dcf719204c",
   "metadata": {},
   "source": [
    "# MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e480743-5fca-46e6-9ef3-d21a6d2cb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mri_tests(seed):\n",
    "    \n",
    "    MRI_TRAIN, MRI_TRAIN_NAME, MRI_TEST, MRI_TEST_NAME = split_mri(seed)\n",
    "\n",
    "    ## AUTOENCODER\n",
    "    model,scaler = AE(MRI_TRAIN, epochs = 50)\n",
    "    processed_AE_train = through_AE(MRI_TRAIN, model, scaler)\n",
    "    processed_AE_test = through_AE(MRI_TEST, model, scaler)\n",
    "    \n",
    "    mri_ae_rf_acc, mri_ae_rf_tbl = random_forest(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])\n",
    "    # return (mri_ae_rf_acc)\n",
    "    mri_ae_lr_acc, mri_ae_lr_tbl = logistic_regression(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])\n",
    "    mri_ae_nn_acc, mri_ae_nn_tbl = neural_network(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])\n",
    "    \n",
    "    ## PCA\n",
    "    pca_train, pca_test = pca(train_df = MRI_TRAIN, test_df = MRI_TEST, var = 0.3)\n",
    "\n",
    "    mri_pca_rf_acc, mri_pca_rf_tbl = random_forest(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "    mri_pca_lr_acc, mri_pca_lr_tbl = logistic_regression( train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "    mri_pca_nn_acc, mri_pca_nn_tbl = neural_network( train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "    \n",
    "    ## LASSO\n",
    "    cols = lasso(df = MRI_TRAIN, al = 0.01, thresh = 0.05)\n",
    "\n",
    "    mri_lasso_rf_acc, mri_lasso_rf_tbl = random_forest( train_df = MRI_TRAIN, test_df = MRI_TEST, cols = cols)\n",
    "    mri_lasso_lr_acc, mri_lasso_lr_tbl = logistic_regression( train_df = MRI_TRAIN, test_df = MRI_TEST, cols = cols)\n",
    "    mri_lasso_nn_acc, mri_lasso_nn_tbl = neural_network(train_df = MRI_TRAIN, test_df = MRI_TEST, cols = cols)\n",
    "    \n",
    "    ## SE\n",
    "    mod, scaler = SDNN(MRI_TRAIN)\n",
    "\n",
    "    processed_df_train = through_SDNN(MRI_TRAIN, mod, scaler)\n",
    "    processed_df_test = through_SDNN(MRI_TEST, mod, scaler)\n",
    "\n",
    "    processed_df_train.columns = processed_df_train.columns.astype(str)\n",
    "    processed_df_test.columns = processed_df_test.columns.astype(str)\n",
    "\n",
    "    mri_sdnn_rf_acc, mri_sdnn_rf_tbl = random_forest( train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    mri_sdnn_lr_acc, mri_sdnn_lr_tbl = logistic_regression(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    mri_sdnn_nn_acc, mri_sdnn_nn_tbl = neural_network( train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    import numpy as np\n",
    "    pca_train, pca_test = pca(train_df = MRI_TRAIN, test_df = MRI_TEST)\n",
    "    MULTICLASS = (\n",
    "        mri_sdnn_rf_tbl,\n",
    "        mri_sdnn_nn_tbl+1,\n",
    "        mri_lasso_rf_tbl,\n",
    "        mri_lasso_nn_tbl+1,\n",
    "        mri_pca_rf_tbl,\n",
    "        mri_pca_nn_tbl+1,\n",
    "        mri_ae_rf_tbl,\n",
    "        mri_ae_nn_tbl+1,\n",
    "    )\n",
    "\n",
    "\n",
    "    BINARY = (\n",
    "        mri_sdnn_lr_tbl,\n",
    "        mri_lasso_lr_tbl,\n",
    "        mri_pca_lr_tbl,\n",
    "        mri_ae_lr_tbl,\n",
    "    )\n",
    "\n",
    "    def do_thing(group, truth):\n",
    "\n",
    "        # Combine input arrays into a single 2D array\n",
    "        input_arrays = np.stack(group)\n",
    "\n",
    "        def find_mode_with_tie_breaker(x):\n",
    "            counts = np.bincount(x)\n",
    "            max_count = np.max(counts)\n",
    "            max_value = np.max([val for val, count in enumerate(counts) if count == max_count])\n",
    "            return max_value\n",
    "\n",
    "        # Apply the function along axis 0 (columns) of the input arrays\n",
    "        voting_result = np.apply_along_axis(find_mode_with_tie_breaker, axis=0, arr=input_arrays)\n",
    "\n",
    "        print(voting_result)\n",
    "\n",
    "        # Calculate the number of matching elements\n",
    "        matching_elements = np.sum(voting_result == truth)\n",
    "\n",
    "        # Calculate the total number of elements\n",
    "        total_elements = len(voting_result)\n",
    "\n",
    "        # Calculate the percentage of matching values\n",
    "        percentage_matching = (matching_elements / total_elements) * 100\n",
    "\n",
    "        # print(f\"Percentage of matching values: {percentage_matching:.2f}%\")\n",
    "        return percentage_matching\n",
    "\n",
    "    multiclass_ensemble_acc = do_thing(MULTICLASS, MRI_TEST[\"NACCUDSD\"])\n",
    "    binary_ensemble_acc = do_thing(BINARY, (MRI_TEST[\"NACCUDSD\"] != 1).astype(int))\n",
    "    \n",
    "    \n",
    "\n",
    "    return [[mri_pca_lr_acc, mri_pca_rf_acc, mri_pca_nn_acc],\n",
    "            [mri_ae_lr_acc, mri_ae_rf_acc, mri_ae_nn_acc],\n",
    "           [mri_lasso_lr_acc, mri_lasso_rf_acc, mri_lasso_nn_acc],\n",
    "            [mri_sdnn_lr_acc, mri_sdnn_rf_acc, mri_sdnn_nn_acc], multiclass_ensemble_acc, binary_ensemble_acc]\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72eb37e3-a5dd-4b6f-9a4a-6cce7a90cb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[[0.7995169082125604, 0.7028985507246377, 0.729468584060669],\\n [0.7560386473429952, 0.6497584541062802, 0.6859903335571289],\\n [0.785024154589372, 0.717391304347826, 0.7149758338928223],\\n [0.7922705314009661, 0.714975845410628, 0.6980676054954529],\\n 74.8792270531401,\\n 80.43478260869566]\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_mri_tests(42) achieves the following, which is similar-ish to current results.\n",
    "\"\"\"\n",
    "[[0.7995169082125604, 0.7028985507246377, 0.729468584060669],\n",
    " [0.7560386473429952, 0.6497584541062802, 0.6859903335571289],\n",
    " [0.785024154589372, 0.717391304347826, 0.7149758338928223],\n",
    " [0.7922705314009661, 0.714975845410628, 0.6980676054954529],\n",
    " 74.8792270531401,\n",
    " 80.43478260869566]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93218d-1352-4571-85e1-861e4e4c5778",
   "metadata": {},
   "source": [
    "# UDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62e1c0b4-654c-400e-8b6b-58a06ba4cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_uds_tests(seed):\n",
    "    \n",
    "    UDS_TRAIN, UDS_TRAIN_NAME, UDS_TEST, UDS_TEST_NAME = split_uds(seed)\n",
    "\n",
    "    pca_train, pca_test = pca(train_df = UDS_TRAIN, test_df = UDS_TEST)\n",
    "\n",
    "    uds_pca_rf_acc, uds_pca_rf_tbl = random_forest(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "    uds_pca_lr_acc, uds_pca_lr_tbl = logistic_regression(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "    uds_pca_nn_acc, uds_pca_nn_tbl = neural_network(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "\n",
    "    cols = lasso(df = UDS_TRAIN, al = 0.01, thresh = 0.04)\n",
    "    uds_lasso_rf_acc, uds_lasso_rf_tbl = random_forest(train_df = UDS_TRAIN, test_df = UDS_TEST, cols = cols)\n",
    "    uds_lasso_lr_acc, uds_lasso_lr_tbl = logistic_regression(train_df = UDS_TRAIN, test_df = UDS_TEST, cols = cols)\n",
    "    uds_lasso_nn_acc, uds_lasso_nn_tbl = neural_network(train_df = UDS_TRAIN, test_df = UDS_TEST, cols = cols)\n",
    "\n",
    "    mod,scaler = SDNN(UDS_TRAIN)\n",
    "    processed_df_train = through_SDNN(UDS_TRAIN, mod, scaler)\n",
    "    processed_df_test = through_SDNN(UDS_TEST, mod, scaler)\n",
    "    uds_sdnn_lr_acc, uds_sdnn_lr_tbl = logistic_regression(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    uds_sdnn_rf_acc, uds_sdnn_rf_tbl = random_forest(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    uds_sdnn_nn_acc, uds_sdnn_nn_tbl = neural_network(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1]) \n",
    "\n",
    "    model, scaler = AE(UDS_TRAIN, epochs = 50)\n",
    "    processed_AE_train = through_AE(UDS_TRAIN, model, scaler)\n",
    "    processed_AE_test = through_AE(UDS_TEST, model, scaler)\n",
    "    uds_ae_lr_acc, uds_ae_lr_tbl = logistic_regression(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])\n",
    "    uds_ae_nn_acc, uds_ae_nn_tbl = neural_network(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])\n",
    "    uds_ae_rf_acc, uds_ae_rf_tbl = random_forest(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])\n",
    "    \n",
    "    import numpy as np\n",
    "    pca_train, pca_test = pca(train_df = UDS_TRAIN, test_df = UDS_TEST)\n",
    "    MULTICLASS = (\n",
    "        uds_sdnn_rf_tbl,\n",
    "        uds_sdnn_nn_tbl+1,\n",
    "        uds_lasso_rf_tbl,\n",
    "        uds_lasso_nn_tbl+1,\n",
    "        uds_pca_rf_tbl,\n",
    "        uds_pca_nn_tbl+1,\n",
    "        uds_ae_rf_tbl,\n",
    "        uds_ae_nn_tbl+1,\n",
    "    )\n",
    "\n",
    "\n",
    "    BINARY = (\n",
    "        uds_sdnn_lr_tbl,\n",
    "        uds_lasso_lr_tbl,\n",
    "        uds_pca_lr_tbl,\n",
    "        uds_ae_lr_tbl,\n",
    "    )\n",
    "\n",
    "    def do_thing(group, truth):\n",
    "\n",
    "        # Combine input arrays into a single 2D array\n",
    "        input_arrays = np.stack(group)\n",
    "\n",
    "        def find_mode_with_tie_breaker(x):\n",
    "            counts = np.bincount(x)\n",
    "            max_count = np.max(counts)\n",
    "            max_value = np.max([val for val, count in enumerate(counts) if count == max_count])\n",
    "            return max_value\n",
    "\n",
    "        # Apply the function along axis 0 (columns) of the input arrays\n",
    "        voting_result = np.apply_along_axis(find_mode_with_tie_breaker, axis=0, arr=input_arrays)\n",
    "\n",
    "        print(voting_result)\n",
    "\n",
    "        # Calculate the number of matching elements\n",
    "        matching_elements = np.sum(voting_result == truth)\n",
    "\n",
    "        # Calculate the total number of elements\n",
    "        total_elements = len(voting_result)\n",
    "\n",
    "        # Calculate the percentage of matching values\n",
    "        percentage_matching = (matching_elements / total_elements) * 100\n",
    "\n",
    "        return percentage_matching\n",
    "\n",
    "    multiclass_ensemble_acc = do_thing(MULTICLASS, pca_test[\"NACCUDSD\"])\n",
    "    binary_ensemble_acc = do_thing(BINARY, (pca_test[\"NACCUDSD\"] != 1).astype(int))\n",
    "    \n",
    "    \n",
    "    return [\n",
    "           [uds_pca_lr_acc, uds_pca_rf_acc, uds_pca_nn_acc],\n",
    "            [uds_ae_lr_acc, uds_ae_rf_acc, uds_ae_nn_acc],\n",
    "           [uds_lasso_lr_acc, uds_lasso_rf_acc, uds_lasso_nn_acc],\n",
    "            [uds_sdnn_lr_acc, uds_sdnn_rf_acc, uds_sdnn_nn_acc], multiclass_ensemble_acc, binary_ensemble_acc]\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08e84c29-36ff-4557-bf85-87925da21783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[[0.8574081716879901, 0.7930251754023937, 0.7983904480934143],\\n [0.7263722657862154, 0.6591002888980603, 0.7026413679122925],\\n [0.8609162195625258, 0.7998349153941395, 0.8025175333023071],\\n [0.8801073049938093, 0.819026000825423, 0.8130416870117188],\\n 81.61370202228643,\\n 87.53611225753198]\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_uds_tests(42) gets\n",
    "\"\"\"\n",
    "[[0.8574081716879901, 0.7930251754023937, 0.7983904480934143],\n",
    " [0.7263722657862154, 0.6591002888980603, 0.7026413679122925],\n",
    " [0.8609162195625258, 0.7998349153941395, 0.8025175333023071],\n",
    " [0.8801073049938093, 0.819026000825423, 0.8130416870117188],\n",
    " 81.61370202228643,\n",
    " 87.53611225753198]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bfc419-0ab7-4019-8b4e-db5b563724cf",
   "metadata": {},
   "source": [
    "# CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eac83e91-fbcb-44c5-af6f-c6472e31e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_csf_tests(seed):\n",
    "    \n",
    "    CSF_TRAIN, CSF_TRAIN_NAME, CSF_TEST, CSF_TEST_NAME = split_csf(seed)\n",
    "    \n",
    "    csf_lr_acc, csf_lr_tbl = logistic_regression(train_df = CSF_TRAIN, test_df = CSF_TEST, cols = ['CSFABETA', 'CSFPTAU', 'CSFTTAU'])\n",
    "    csf_rf_acc, csf_rf_tbl = random_forest( train_df = CSF_TRAIN, test_df = CSF_TEST, cols = ['CSFABETA', 'CSFPTAU', 'CSFTTAU'])\n",
    "    csf_nn_acc, csf_nn_tbl =  neural_network( train_df = CSF_TRAIN, test_df = CSF_TEST, cols = ['CSFABETA', 'CSFPTAU', 'CSFTTAU'])\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    MULTICLASS = (\n",
    "        csf_nn_tbl+1,\n",
    "        csf_rf_tbl\n",
    "    )\n",
    "\n",
    "\n",
    "    def do_thing(group, truth):\n",
    "\n",
    "        # Combine input arrays into a single 2D array\n",
    "        input_arrays = np.stack(group)\n",
    "\n",
    "        def find_mode_with_tie_breaker(x):\n",
    "            counts = np.bincount(x)\n",
    "            max_count = np.max(counts)\n",
    "            max_value = np.max([val for val, count in enumerate(counts) if count == max_count])\n",
    "            return max_value\n",
    "\n",
    "        # Apply the function along axis 0 (columns) of the input arrays\n",
    "        voting_result = np.apply_along_axis(find_mode_with_tie_breaker, axis=0, arr=input_arrays)\n",
    "\n",
    "        print(voting_result)\n",
    "\n",
    "        # Calculate the number of matching elements\n",
    "        matching_elements = np.sum(voting_result == truth)\n",
    "\n",
    "        # Calculate the total number of elements\n",
    "        total_elements = len(voting_result)\n",
    "\n",
    "        # Calculate the percentage of matching values\n",
    "        percentage_matching = (matching_elements / total_elements) * 100\n",
    "\n",
    "        return percentage_matching\n",
    "\n",
    "    multiclass_ensemble_acc = do_thing(MULTICLASS, CSF_TEST['NACCUDSD'])\n",
    "\n",
    "    \n",
    "    return [[csf_lr_acc, csf_rf_acc, csf_nn_acc], multiclass_ensemble_acc]\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cd3f2f8-d6e2-49fc-a606-faed16e68534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[[0.8169761273209549, 0.7374005305039788, 0.7824933528900146],\\n 76.39257294429707]\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_csf_tests(42) gets\n",
    "\"\"\"\n",
    "[[0.8169761273209549, 0.7374005305039788, 0.7824933528900146],\n",
    " 76.39257294429707]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a46b33-b1e4-4ead-9181-902f5fe3a4c6",
   "metadata": {},
   "source": [
    "# Simple Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e917aa2-a83a-4be9-b5f8-7b3940f387e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_fusion_tests(seed):\n",
    "    \n",
    "    UDS_MRI_TRAIN, UDS_MRI_TRAIN_NAME, UDS_MRI_TEST, UDS_MRI_TEST_NAME = split_uds_mri(seed)\n",
    "    \n",
    "    uds_mri_s_lr_acc, uds_mri_s_lr_tbl = logistic_regression(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_MRI_TRAIN.columns[:-1])\n",
    "    uds_mri_s_rf_acc, uds_mri_s_rf_tbl = random_forest(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_MRI_TRAIN.columns[:-1])\n",
    "    uds_mri_s_nn_acc, uds_mri_s_nn_tbl = neural_network(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_MRI_TRAIN.columns[:-1])\n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    MULTICLASS = (\n",
    "        uds_mri_s_nn_tbl+1,\n",
    "        uds_mri_s_rf_tbl\n",
    "    )\n",
    "\n",
    "\n",
    "    def do_thing(group, truth):\n",
    "\n",
    "        # Combine input arrays into a single 2D array\n",
    "        input_arrays = np.stack(group)\n",
    "\n",
    "        def find_mode_with_tie_breaker(x):\n",
    "            counts = np.bincount(x)\n",
    "            max_count = np.max(counts)\n",
    "            max_value = np.max([val for val, count in enumerate(counts) if count == max_count])\n",
    "            return max_value\n",
    "\n",
    "        # Apply the function along axis 0 (columns) of the input arrays\n",
    "        voting_result = np.apply_along_axis(find_mode_with_tie_breaker, axis=0, arr=input_arrays)\n",
    "\n",
    "        print(voting_result)\n",
    "\n",
    "        # Calculate the number of matching elements\n",
    "        matching_elements = np.sum(voting_result == truth)\n",
    "\n",
    "        # Calculate the total number of elements\n",
    "        total_elements = len(voting_result)\n",
    "\n",
    "        # Calculate the percentage of matching values\n",
    "        percentage_matching = (matching_elements / total_elements) * 100\n",
    "\n",
    "        return percentage_matching\n",
    "\n",
    "    multiclass_ensemble_acc = do_thing(MULTICLASS, UDS_MRI_TEST['NACCUDSD'])\n",
    "    \n",
    "    return [[uds_mri_s_lr_acc, uds_mri_s_rf_acc, uds_mri_s_nn_acc], multiclass_ensemble_acc]\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6293b6f1-b9a5-4443-8c65-1519483896fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m UDS_MRI_TRAIN, UDS_MRI_TRAIN_NAME, UDS_MRI_TEST, UDS_MRI_TEST_NAME \u001b[38;5;241m=\u001b[39m split_uds_mri(\u001b[43mseed\u001b[49m)\n\u001b[0;32m      2\u001b[0m uds_mri_s_rf_tbl \u001b[38;5;241m=\u001b[39m random_forest(give_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, train_df \u001b[38;5;241m=\u001b[39m UDS_MRI_TRAIN, test_df \u001b[38;5;241m=\u001b[39m UDS_MRI_TEST, cols \u001b[38;5;241m=\u001b[39m UDS_MRI_TRAIN\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seed' is not defined"
     ]
    }
   ],
   "source": [
    "UDS_MRI_TRAIN, UDS_MRI_TRAIN_NAME, UDS_MRI_TEST, UDS_MRI_TEST_NAME = split_uds_mri(seed)\n",
    "uds_mri_s_rf_tbl = random_forest(give_table = True, train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_MRI_TRAIN.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e88a997b-6902-4622-b48d-176c6c975ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[[0.7927170868347339, 0.8263305322128851, 0.8067227005958557],\\n 83.19327731092437]\\n '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_simple_fusion_tests(42) gets \n",
    "\"\"\"\n",
    "[[0.7927170868347339, 0.8263305322128851, 0.8067227005958557],\n",
    " 83.19327731092437]\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21f30adc-7fd4-4c9f-9a43-a5d50bd70f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_early_fusion_tests(seed):\n",
    "    \n",
    "    UDS_MRI_TRAIN, UDS_MRI_TRAIN_NAME, UDS_MRI_TEST, UDS_MRI_TEST_NAME = split_uds_mri(seed)\n",
    "    \n",
    "    pca_train, pca_test = pca(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST)\n",
    "    uds_mri_e_pca_rf_acc, uds_mri_e_pca_rf_tbl = random_forest( train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "    uds_mri_e_pca_lr_acc, uds_mri_e_pca_lr_tbl = logistic_regression( train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])\n",
    "    uds_mri_e_pca_nn_acc, uds_mri_e_pca_nn_tbl = neural_network(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8], epochs = 15)\n",
    "\n",
    "    them_cols = lasso(df = UDS_MRI_TRAIN, al = 0.05, thresh = 0.05) # al & thresh .05 worked best\n",
    "    uds_mri_e_lasso_lr_acc, uds_mri_e_lasso_lr_tbl = logistic_regression( train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = them_cols)\n",
    "    uds_mri_e_lasso_rf_acc, uds_mri_e_lasso_rf_tbl = random_forest( train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = lasso(df = UDS_MRI_TRAIN, al = 2 / 100, thresh = 1 / 100))\n",
    "    uds_mri_e_lasso_nn_acc, uds_mri_e_lasso_nn_tbl = neural_network( train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = them_cols)\n",
    "\n",
    "    mod, scaler = SDNN(UDS_MRI_TRAIN)\n",
    "    processed_df_train = through_SDNN(UDS_MRI_TRAIN, mod, scaler)\n",
    "    processed_df_test = through_SDNN(UDS_MRI_TEST, mod, scaler)\n",
    "    processed_df_train.columns = processed_df_train.columns.astype(str)\n",
    "    processed_df_test.columns = processed_df_test.columns.astype(str)\n",
    "    uds_mri_e_sdnn_rf_acc, uds_mri_e_sdnn_rf_tbl = random_forest( train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    uds_mri_e_sdnn_lr_acc, uds_mri_e_sdnn_lr_tbl = logistic_regression( train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    uds_mri_e_sdnn_nn_acc, uds_mri_e_sdnn_nn_tbl = neural_network( train_df = processed_df_train, test_df = processed_df_test, epochs = 3, cols = processed_df_train.columns[:-1])\n",
    "\n",
    "    mod, scaler = AE(UDS_MRI_TRAIN, epochs = 50)\n",
    "    processed_df_train = through_AE(UDS_MRI_TRAIN, mod, scaler)\n",
    "    processed_df_test = through_AE(UDS_MRI_TEST, mod, scaler)\n",
    "    processed_df_train.columns = processed_df_train.columns.astype(str)\n",
    "    processed_df_test.columns = processed_df_test.columns.astype(str)\n",
    "    uds_mri_e_ae_lr_acc, uds_mri_e_ae_lr_tbl = logistic_regression( train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    uds_mri_e_ae_rf_acc, uds_mri_e_ae_rf_tbl = random_forest( train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "    uds_mri_e_ae_nn_acc, uds_mri_e_ae_nn_tbl = neural_network( train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])\n",
    "\n",
    "    import numpy as np\n",
    "    pca_train, pca_test = pca(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST)\n",
    "    MULTICLASS = (\n",
    "        uds_mri_e_ae_nn_tbl+1,\n",
    "        uds_mri_e_ae_rf_tbl,\n",
    "        uds_mri_e_lasso_nn_tbl+1,\n",
    "        uds_mri_e_lasso_rf_tbl,\n",
    "        uds_mri_e_pca_nn_tbl+1,\n",
    "        uds_mri_e_pca_rf_tbl,\n",
    "        uds_mri_e_sdnn_nn_tbl+1,\n",
    "        uds_mri_e_sdnn_rf_tbl,\n",
    "    )\n",
    "\n",
    "    BINARY = (\n",
    "        uds_mri_e_ae_lr_tbl,\n",
    "        uds_mri_e_lasso_lr_tbl,\n",
    "        uds_mri_e_pca_lr_tbl,\n",
    "        uds_mri_e_sdnn_lr_tbl\n",
    "    )\n",
    "\n",
    "    def do_thing(group, truth):\n",
    "\n",
    "        # Combine input arrays into a single 2D array\n",
    "        input_arrays = np.stack(group)\n",
    "\n",
    "        def find_mode_with_tie_breaker(x):\n",
    "            counts = np.bincount(x)\n",
    "            max_count = np.max(counts)\n",
    "            max_value = np.max([val for val, count in enumerate(counts) if count == max_count])\n",
    "            return max_value\n",
    "\n",
    "        # Apply the function along axis 0 (columns) of the input arrays\n",
    "        voting_result = np.apply_along_axis(find_mode_with_tie_breaker, axis=0, arr=input_arrays)\n",
    "\n",
    "        print(voting_result)\n",
    "\n",
    "        # Calculate the number of matching elements\n",
    "        matching_elements = np.sum(voting_result == truth)\n",
    "\n",
    "        # Calculate the total number of elements\n",
    "        total_elements = len(voting_result)\n",
    "\n",
    "        # Calculate the percentage of matching values\n",
    "        percentage_matching = (matching_elements / total_elements) * 100\n",
    "\n",
    "        return percentage_matching\n",
    "\n",
    "    multiclass_ensemble_acc = do_thing(MULTICLASS, pca_test[\"NACCUDSD\"])\n",
    "    binary_ensemble_acc = do_thing(BINARY, (pca_test[\"NACCUDSD\"] != 1).astype(int))\n",
    "    \n",
    "    return [[uds_mri_e_pca_lr_acc, uds_mri_e_pca_rf_acc, uds_mri_e_pca_nn_acc],\n",
    "            [uds_mri_e_ae_lr_acc, uds_mri_e_ae_rf_acc, uds_mri_e_ae_nn_acc],\n",
    "            [uds_mri_e_lasso_lr_acc, uds_mri_e_lasso_rf_acc, uds_mri_e_lasso_nn_acc],\n",
    "           [uds_mri_e_sdnn_lr_acc, uds_mri_e_sdnn_rf_acc, uds_mri_e_sdnn_nn_acc],\n",
    "             multiclass_ensemble_acc, binary_ensemble_acc]\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "000f5f4a-79b1-4b8b-b244-70b8bb89c043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[[0.8375350140056023, 0.7815126050420168, 0.7703081369400024],\\n [0.7983193277310925, 0.7310924369747899, 0.756302535533905],\\n [0.8879551820728291, 0.8319327731092437, 0.8347339034080505],\\n [0.8823529411764706, 0.7927170868347339, 0.7983193397521973],\\n 80.3921568627451,\\n 88.51540616246498]\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_early_fusion_tests(42) gets\n",
    "\"\"\"\n",
    "[[0.8375350140056023, 0.7815126050420168, 0.7703081369400024],\n",
    " [0.7983193277310925, 0.7310924369747899, 0.756302535533905],\n",
    " [0.8879551820728291, 0.8319327731092437, 0.8347339034080505],\n",
    " [0.8823529411764706, 0.7927170868347339, 0.7983193397521973],\n",
    " 80.3921568627451,\n",
    " 88.51540616246498]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b3bc19c-5d27-4d42-af0e-ae49c48841b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_late_fusion_tests(seed):\n",
    "    # This is a bit weird, but due to my data pipeline the merged UDS_MRI doesn't have the same features as UDS and MRI.\n",
    "    # That was to preserve more data in each dataset. So here we are just finding columns in common.\n",
    "    \n",
    "    UDS_MRI_TRAIN, UDS_MRI_TRAIN_NAME, UDS_MRI_TEST, UDS_MRI_TEST_NAME = split_uds_mri(seed)\n",
    "    UDS_TRAIN, UDS_TRAIN_NAME, UDS_TEST, UDS_TEST_NAME = split_uds(seed)\n",
    "    MRI_TRAIN, MRI_TRAIN_NAME, MRI_TEST, MRI_TEST_NAME = split_mri(seed)\n",
    "\n",
    "    UDS_cols_in_merged = [x for x in UDS_MRI_TRAIN.columns if x not in input_mri_feats]\n",
    "    UDS_cols_use = [x for x in UDS_cols_in_merged if x in UDS_TRAIN.columns]\n",
    "\n",
    "    MRI_cols_in_merged = [x for x in UDS_MRI_TRAIN.columns if x not in UDS_cols_in_merged]\n",
    "    MRI_cols_use = [x for x in MRI_cols_in_merged if x in MRI_TRAIN.columns] + ['NACCUDSD']\n",
    "    \n",
    "    UDS_mod, UDS_scaler = SDNN(UDS_TRAIN[UDS_cols_use])\n",
    "    MRI_mod, MRI_scaler = SDNN(MRI_TRAIN[MRI_cols_use])\n",
    "    processed_df_UDS_train = through_SDNN(UDS_MRI_TRAIN[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "    processed_df_UDS_test = through_SDNN(UDS_MRI_TEST[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "    processed_df_MRI_train = through_SDNN(UDS_MRI_TRAIN[MRI_cols_use], MRI_mod, MRI_scaler)\n",
    "    processed_df_MRI_test = through_SDNN(UDS_MRI_TEST[MRI_cols_use], MRI_mod , MRI_scaler)\n",
    "    merged_sdnn_train = processed_df_UDS_train[processed_df_UDS_train.columns[:-1]].merge(processed_df_MRI_train[processed_df_MRI_train.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "    merged_sdnn_train['NACCUDSD'] = processed_df_UDS_train['NACCUDSD']\n",
    "    merged_sdnn_train\n",
    "    merged_sdnn_test = processed_df_UDS_test[processed_df_UDS_test.columns[:-1]].merge(processed_df_MRI_test[processed_df_MRI_test.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "    merged_sdnn_test['NACCUDSD'] = processed_df_UDS_test['NACCUDSD']\n",
    "    uds_mri_l_sdnn_lr_acc, uds_mri_l_sdnn_lr_tbl = logistic_regression( train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])\n",
    "    uds_mri_l_sdnn_rf_acc, uds_mri_l_sdnn_rf_tbl = random_forest( train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])\n",
    "    uds_mri_l_sdnn_nn_acc, uds_mri_l_sdnn_nn_tbl = neural_network( train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])\n",
    "\n",
    "    UDS_mod, UDS_scaler = AE(UDS_TRAIN[UDS_cols_use], epochs = 50)\n",
    "    MRI_mod, MRI_scaler = AE(MRI_TRAIN[MRI_cols_use], epochs = 50)\n",
    "    processed_df_UDS_train = through_AE(UDS_MRI_TRAIN[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "    processed_df_UDS_test = through_AE(UDS_MRI_TEST[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "    processed_df_MRI_train = through_AE(UDS_MRI_TRAIN[MRI_cols_use], MRI_mod, MRI_scaler)\n",
    "    processed_df_MRI_test = through_AE(UDS_MRI_TEST[MRI_cols_use], MRI_mod, MRI_scaler)\n",
    "    merged_ae_train = processed_df_UDS_train[processed_df_UDS_train.columns[:-1]].merge(processed_df_MRI_train[processed_df_MRI_train.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "    merged_ae_train['NACCUDSD'] = processed_df_UDS_train['NACCUDSD']\n",
    "    merged_ae_test = processed_df_UDS_test[processed_df_UDS_test.columns[:-1]].merge(processed_df_MRI_test[processed_df_MRI_test.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "    merged_ae_test['NACCUDSD'] = processed_df_UDS_test['NACCUDSD']\n",
    "\n",
    "    uds_mri_l_ae_lr_acc, uds_mri_l_ae_lr_tbl = logistic_regression( train_df = merged_ae_train, test_df = merged_ae_test, cols = merged_ae_train.columns[:-1])\n",
    "    uds_mri_l_ae_rf_acc, uds_mri_l_ae_rf_tbl = random_forest( train_df = merged_ae_train, test_df = merged_ae_test, cols = merged_ae_train.columns[:-1])\n",
    "    uds_mri_l_ae_nn_acc, uds_mri_l_ae_nn_tbl = neural_network( train_df = merged_ae_train, test_df = merged_ae_test, cols = merged_ae_train.columns[:-1], epochs = 50)\n",
    "    \n",
    "    pca_UDS_train, pca_UDS_test = pca(train_df = UDS_MRI_TRAIN[UDS_cols_use], test_df = UDS_MRI_TEST[UDS_cols_use])\n",
    "    pca_MRI_train, pca_MRI_test = pca(train_df = UDS_MRI_TRAIN[MRI_cols_use], test_df = UDS_MRI_TEST[MRI_cols_use])\n",
    "    merged_pca_train = pca_MRI_train[pca_MRI_train.columns[:8]].merge(pca_UDS_train[pca_UDS_train.columns[:8]], how = \"outer\", left_index=True, right_index=True)\n",
    "    merged_pca_train['NACCUDSD'] = pca_MRI_train['NACCUDSD']\n",
    "    merged_pca_train\n",
    "    merged_pca_test = pca_MRI_test[pca_MRI_test.columns[:8]].merge(pca_UDS_test[pca_UDS_test.columns[:8]], how = \"outer\", left_index=True, right_index=True)\n",
    "    merged_pca_test['NACCUDSD'] = pca_MRI_test['NACCUDSD']\n",
    "    merged_pca_test\n",
    "    uds_mri_l_pca_rf_acc, uds_mri_l_pca_rf_tbl = random_forest( train_df = merged_pca_train, test_df = merged_pca_test, cols = merged_pca_train.columns[:-1])\n",
    "    uds_mri_l_pca_lr_acc, uds_mri_l_pca_lr_tbl = logistic_regression( train_df = merged_pca_train, test_df = merged_pca_test, cols = merged_pca_train.columns[:-1])\n",
    "    uds_mri_l_pca_nn_acc, uds_mri_l_pca_nn_tbl = neural_network( train_df = merged_pca_train, test_df = merged_pca_test, cols = merged_pca_train.columns[:-1], epochs = 20)\n",
    "\n",
    "    \n",
    "    UDS_cols = lasso(df = UDS_TRAIN, al = 0.01, thresh = 0.04)\n",
    "    UDS_cols\n",
    "    MRI_cols = lasso(df = MRI_TRAIN, al = 0.01, thresh = 0.04)\n",
    "    MRI_cols\n",
    "    uds_mri_l_lasso_lr_acc, uds_mri_l_lasso_lr_tbl = logistic_regression( train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_cols+MRI_cols)\n",
    "    uds_mri_l_lasso_rf_acc, uds_mri_l_lasso_rf_tbl = random_forest( train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_cols+MRI_cols)\n",
    "    uds_mri_l_lasso_nn_acc, uds_mri_l_lasso_nn_tbl = neural_network( train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_cols+MRI_cols, epochs = 15)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    print(uds_mri_l_sdnn_lr_acc)\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    MULTICLASS = (\n",
    "        uds_mri_l_ae_nn_tbl+1,\n",
    "        uds_mri_l_ae_rf_tbl,\n",
    "        uds_mri_l_lasso_nn_tbl+1,\n",
    "        uds_mri_l_lasso_rf_tbl,\n",
    "        uds_mri_l_pca_nn_tbl+1,\n",
    "        uds_mri_l_pca_rf_tbl,\n",
    "        uds_mri_l_sdnn_nn_tbl+1,\n",
    "        uds_mri_l_sdnn_rf_tbl,\n",
    "    )\n",
    "\n",
    "    BINARY = (\n",
    "        uds_mri_l_ae_lr_tbl,\n",
    "        uds_mri_l_lasso_lr_tbl,\n",
    "        uds_mri_l_pca_lr_tbl,\n",
    "        uds_mri_l_sdnn_lr_tbl\n",
    "    )\n",
    "\n",
    "    def do_thing(group, truth):\n",
    "\n",
    "        # Combine input arrays into a single 2D array\n",
    "        input_arrays = np.stack(group)\n",
    "\n",
    "        def find_mode_with_tie_breaker(x):\n",
    "            counts = np.bincount(x)\n",
    "            max_count = np.max(counts)\n",
    "            max_value = np.max([val for val, count in enumerate(counts) if count == max_count])\n",
    "            return max_value\n",
    "\n",
    "        # Apply the function along axis 0 (columns) of the input arrays\n",
    "        voting_result = np.apply_along_axis(find_mode_with_tie_breaker, axis=0, arr=input_arrays)\n",
    "\n",
    "        print(voting_result)\n",
    "\n",
    "        # Calculate the number of matching elements\n",
    "        matching_elements = np.sum(voting_result == truth)\n",
    "\n",
    "        # Calculate the total number of elements\n",
    "        total_elements = len(voting_result)\n",
    "\n",
    "        # Calculate the percentage of matching values\n",
    "        percentage_matching = (matching_elements / total_elements) * 100\n",
    "\n",
    "        return percentage_matching\n",
    "\n",
    "    multiclass_ensemble_acc = do_thing(MULTICLASS, merged_pca_test['NACCUDSD'])\n",
    "    binary_ensemble_acc = do_thing(BINARY, (merged_pca_test['NACCUDSD'] != 1).astype(int))\n",
    "    \n",
    "    return [[uds_mri_l_pca_lr_acc, uds_mri_l_pca_rf_acc, uds_mri_l_pca_nn_acc],\n",
    "[uds_mri_l_ae_lr_acc, uds_mri_l_ae_rf_acc, uds_mri_l_ae_nn_acc],\n",
    "[uds_mri_l_lasso_lr_acc, uds_mri_l_lasso_rf_acc, uds_mri_l_lasso_nn_acc],\n",
    "[uds_mri_l_sdnn_lr_acc, uds_mri_l_sdnn_rf_acc, uds_mri_l_sdnn_nn_acc], multiclass_ensemble_acc, binary_ensemble_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61ef677d-b2a1-4dee-803c-341f753b01d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (1068, 265)\n",
      "Test set shape: (357, 265)\n",
      "Train set shape: (14535, 110)\n",
      "Test set shape: (4846, 110)\n",
      "Train set shape: (1239, 156)\n",
      "Test set shape: (414, 156)\n",
      "Epoch 1/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.5659 - accuracy: 0.7928\n",
      "Epoch 2/10\n",
      "909/909 [==============================] - 1s 2ms/step - loss: 0.4647 - accuracy: 0.8261\n",
      "Epoch 3/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.4360 - accuracy: 0.8380\n",
      "Epoch 4/10\n",
      "909/909 [==============================] - 1s 2ms/step - loss: 0.4144 - accuracy: 0.8446\n",
      "Epoch 5/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3982 - accuracy: 0.8535\n",
      "Epoch 6/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3799 - accuracy: 0.8567\n",
      "Epoch 7/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3639 - accuracy: 0.8632\n",
      "Epoch 8/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3488 - accuracy: 0.8718\n",
      "Epoch 9/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3326 - accuracy: 0.8748\n",
      "Epoch 10/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3178 - accuracy: 0.8795\n",
      "Epoch 1/10\n",
      "78/78 [==============================] - 1s 2ms/step - loss: 0.9956 - accuracy: 0.6077\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.7246 - accuracy: 0.6965\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.6344 - accuracy: 0.7393\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.5837 - accuracy: 0.7635\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7998\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.8249\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8418\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3743 - accuracy: 0.8636\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8935\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.9007\n",
      "0.9243697478991597\n",
      "0.8823529411764706\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 1s 2ms/step - loss: 0.9367 - accuracy: 0.6526\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.8352\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8605\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8670\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8661\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.8699\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.8745\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3286 - accuracy: 0.8745\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3175 - accuracy: 0.8830\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8830\n",
      "Test loss: 0.327835351228714\n",
      "Test accuracy: 0.8935574293136597\n",
      "12/12 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9243697478991597, 0.8823529411764706, 0.8935574293136597)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 4829\n",
    "UDS_MRI_TRAIN, UDS_MRI_TRAIN_NAME, UDS_MRI_TEST, UDS_MRI_TEST_NAME = split_uds_mri(seed)\n",
    "UDS_TRAIN, UDS_TRAIN_NAME, UDS_TEST, UDS_TEST_NAME = split_uds(seed)\n",
    "MRI_TRAIN, MRI_TRAIN_NAME, MRI_TEST, MRI_TEST_NAME = split_mri(seed)\n",
    "\n",
    "UDS_cols_in_merged = [x for x in UDS_MRI_TRAIN.columns if x not in input_mri_feats]\n",
    "UDS_cols_use = [x for x in UDS_cols_in_merged if x in UDS_TRAIN.columns]\n",
    "\n",
    "MRI_cols_in_merged = [x for x in UDS_MRI_TRAIN.columns if x not in UDS_cols_in_merged]\n",
    "MRI_cols_use = [x for x in MRI_cols_in_merged if x in MRI_TRAIN.columns] + ['NACCUDSD']\n",
    "\n",
    "\n",
    "UDS_mod, UDS_scaler = SDNN(UDS_TRAIN[UDS_cols_use])\n",
    "MRI_mod, MRI_scaler = SDNN(MRI_TRAIN[MRI_cols_use])\n",
    "processed_df_UDS_train = through_SDNN(UDS_MRI_TRAIN[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "processed_df_UDS_test = through_SDNN(UDS_MRI_TEST[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "processed_df_MRI_train = through_SDNN(UDS_MRI_TRAIN[MRI_cols_use], MRI_mod, MRI_scaler)\n",
    "processed_df_MRI_test = through_SDNN(UDS_MRI_TEST[MRI_cols_use], MRI_mod , MRI_scaler)\n",
    "merged_sdnn_train = processed_df_UDS_train[processed_df_UDS_train.columns[:-1]].merge(processed_df_MRI_train[processed_df_MRI_train.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_sdnn_train['NACCUDSD'] = processed_df_UDS_train['NACCUDSD']\n",
    "merged_sdnn_train\n",
    "merged_sdnn_test = processed_df_UDS_test[processed_df_UDS_test.columns[:-1]].merge(processed_df_MRI_test[processed_df_MRI_test.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_sdnn_test['NACCUDSD'] = processed_df_UDS_test['NACCUDSD']\n",
    "uds_mri_l_sdnn_lr_acc, uds_mri_l_sdnn_lr_tbl = logistic_regression( train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])\n",
    "uds_mri_l_sdnn_rf_acc, uds_mri_l_sdnn_rf_tbl = random_forest( train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])\n",
    "uds_mri_l_sdnn_nn_acc, uds_mri_l_sdnn_nn_tbl = neural_network( train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])\n",
    "\n",
    "uds_mri_l_sdnn_lr_acc, uds_mri_l_sdnn_rf_acc, uds_mri_l_sdnn_nn_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15130b96-cb54-4de2-882f-6fc5cce50906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (1068, 265)\n",
      "Test set shape: (357, 265)\n",
      "Train set shape: (14535, 110)\n",
      "Test set shape: (4846, 110)\n",
      "Train set shape: (1239, 156)\n",
      "Test set shape: (414, 156)\n",
      "Epoch 1/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.5955 - accuracy: 0.7777\n",
      "Epoch 2/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.4791 - accuracy: 0.8196\n",
      "Epoch 3/10\n",
      "909/909 [==============================] - 1s 2ms/step - loss: 0.4435 - accuracy: 0.8330\n",
      "Epoch 4/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.4216 - accuracy: 0.8422\n",
      "Epoch 5/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3988 - accuracy: 0.8476\n",
      "Epoch 6/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3848 - accuracy: 0.8531\n",
      "Epoch 7/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3709 - accuracy: 0.8575\n",
      "Epoch 8/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3548 - accuracy: 0.8630\n",
      "Epoch 9/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3381 - accuracy: 0.8722\n",
      "Epoch 10/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3254 - accuracy: 0.8762\n",
      "Epoch 1/10\n",
      "78/78 [==============================] - 1s 2ms/step - loss: 1.1161 - accuracy: 0.5650\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.7859 - accuracy: 0.6933\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.7280\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.7522\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.7716\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7950\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.4526 - accuracy: 0.8321\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8612\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8741\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047619047619048\n",
      "0.8543417366946778\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 1s 2ms/step - loss: 1.0488 - accuracy: 0.6498\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.8287\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8652\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8830\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8867\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3189 - accuracy: 0.8904\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.8942\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3052 - accuracy: 0.8886\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8961\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.8961\n",
      "Test loss: 0.3807145655155182\n",
      "Test accuracy: 0.8599439859390259\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "(109,)\n",
      "Epoch 1/50\n",
      "909/909 [==============================] - 3s 2ms/step - loss: 0.9839 - accuracy: 0.0269\n",
      "Epoch 2/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9795 - accuracy: 0.0451\n",
      "Epoch 3/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9791 - accuracy: 0.0470\n",
      "Epoch 4/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9771 - accuracy: 0.0668\n",
      "Epoch 5/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9757 - accuracy: 0.0753\n",
      "Epoch 6/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9750 - accuracy: 0.0785\n",
      "Epoch 7/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9747 - accuracy: 0.0797\n",
      "Epoch 8/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9744 - accuracy: 0.0833\n",
      "Epoch 9/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9742 - accuracy: 0.0856\n",
      "Epoch 10/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9740 - accuracy: 0.0862\n",
      "Epoch 11/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9739 - accuracy: 0.0861\n",
      "Epoch 12/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9739 - accuracy: 0.0872\n",
      "Epoch 13/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9738 - accuracy: 0.0871\n",
      "Epoch 14/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9738 - accuracy: 0.0870\n",
      "Epoch 15/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9738 - accuracy: 0.0879\n",
      "Epoch 16/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9736 - accuracy: 0.0905\n",
      "Epoch 17/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9730 - accuracy: 0.0948\n",
      "Epoch 18/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9727 - accuracy: 0.0960\n",
      "Epoch 19/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9727 - accuracy: 0.0970\n",
      "Epoch 20/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9726 - accuracy: 0.0971\n",
      "Epoch 21/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9723 - accuracy: 0.1020\n",
      "Epoch 22/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9720 - accuracy: 0.1057\n",
      "Epoch 23/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9718 - accuracy: 0.1101\n",
      "Epoch 24/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9709 - accuracy: 0.1219\n",
      "Epoch 25/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9703 - accuracy: 0.1302\n",
      "Epoch 26/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9702 - accuracy: 0.1326\n",
      "Epoch 27/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9701 - accuracy: 0.1338\n",
      "Epoch 28/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9700 - accuracy: 0.1366\n",
      "Epoch 29/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9700 - accuracy: 0.1375\n",
      "Epoch 30/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9699 - accuracy: 0.1381\n",
      "Epoch 31/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9698 - accuracy: 0.1424\n",
      "Epoch 32/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9692 - accuracy: 0.1536\n",
      "Epoch 33/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9689 - accuracy: 0.1555\n",
      "Epoch 34/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9687 - accuracy: 0.1593\n",
      "Epoch 35/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9685 - accuracy: 0.1595\n",
      "Epoch 36/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9685 - accuracy: 0.1611\n",
      "Epoch 37/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9685 - accuracy: 0.1609\n",
      "Epoch 38/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9685 - accuracy: 0.1609\n",
      "Epoch 39/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9684 - accuracy: 0.1613\n",
      "Epoch 40/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9684 - accuracy: 0.1617\n",
      "Epoch 41/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9684 - accuracy: 0.1619\n",
      "Epoch 42/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9685 - accuracy: 0.1618\n",
      "Epoch 43/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9684 - accuracy: 0.1627\n",
      "Epoch 44/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9684 - accuracy: 0.1617\n",
      "Epoch 45/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9684 - accuracy: 0.1612\n",
      "Epoch 46/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9683 - accuracy: 0.1620\n",
      "Epoch 47/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9680 - accuracy: 0.1686\n",
      "Epoch 48/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9679 - accuracy: 0.1686\n",
      "Epoch 49/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9679 - accuracy: 0.1697\n",
      "Epoch 50/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9679 - accuracy: 0.1696\n",
      "(155,)\n",
      "Epoch 1/50\n",
      "78/78 [==============================] - 1s 2ms/step - loss: 0.9987 - accuracy: 0.0073\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9960 - accuracy: 0.0089\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9936 - accuracy: 0.0113\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9928 - accuracy: 0.0081\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9922 - accuracy: 0.0097\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9913 - accuracy: 0.0113\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9908 - accuracy: 0.0194\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9900 - accuracy: 0.0194\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9895 - accuracy: 0.0218\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9891 - accuracy: 0.0291\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9889 - accuracy: 0.0210\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9888 - accuracy: 0.0266\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9885 - accuracy: 0.0266\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9883 - accuracy: 0.0331\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9881 - accuracy: 0.0355\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9880 - accuracy: 0.0323\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9878 - accuracy: 0.0468\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9877 - accuracy: 0.0363\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9874 - accuracy: 0.0412\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9873 - accuracy: 0.0412\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9871 - accuracy: 0.0452\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9869 - accuracy: 0.0484\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9869 - accuracy: 0.0484\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9867 - accuracy: 0.0492\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9866 - accuracy: 0.0541\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9865 - accuracy: 0.0557\n",
      "Epoch 27/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9864 - accuracy: 0.0573\n",
      "Epoch 28/50\n",
      "78/78 [==============================] - 0s 1ms/step - loss: 0.9863 - accuracy: 0.0573\n",
      "Epoch 29/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9863 - accuracy: 0.0533\n",
      "Epoch 30/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9861 - accuracy: 0.0670\n",
      "Epoch 31/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9860 - accuracy: 0.0589\n",
      "Epoch 32/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9860 - accuracy: 0.0605\n",
      "Epoch 33/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9861 - accuracy: 0.0573\n",
      "Epoch 34/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9859 - accuracy: 0.0630\n",
      "Epoch 35/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9859 - accuracy: 0.0694\n",
      "Epoch 36/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9858 - accuracy: 0.0743\n",
      "Epoch 37/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9857 - accuracy: 0.0702\n",
      "Epoch 38/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.0807\n",
      "Epoch 39/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.0743\n",
      "Epoch 40/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9855 - accuracy: 0.0734\n",
      "Epoch 41/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.0791\n",
      "Epoch 42/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9855 - accuracy: 0.0767\n",
      "Epoch 43/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9855 - accuracy: 0.0807\n",
      "Epoch 44/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9857 - accuracy: 0.0734\n",
      "Epoch 45/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9854 - accuracy: 0.0791\n",
      "Epoch 46/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9853 - accuracy: 0.0799\n",
      "Epoch 47/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9853 - accuracy: 0.0823\n",
      "Epoch 48/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9851 - accuracy: 0.0775\n",
      "Epoch 49/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9852 - accuracy: 0.0807\n",
      "Epoch 50/50\n",
      "78/78 [==============================] - 0s 2ms/step - loss: 0.9851 - accuracy: 0.0872\n",
      "started\n",
      "34/34 [==============================] - 0s 2ms/step\n",
      "started\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "started\n",
      "34/34 [==============================] - 0s 629us/step\n",
      "started\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "0.6246498599439776\n",
      "0.6974789915966386\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 1s 2ms/step - loss: 1.1619 - accuracy: 0.5112\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9487 - accuracy: 0.6199\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9006 - accuracy: 0.6386\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.8746 - accuracy: 0.6451\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8568 - accuracy: 0.6657\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8405 - accuracy: 0.6695\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8310 - accuracy: 0.6704\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.8200 - accuracy: 0.6779\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.8085 - accuracy: 0.6863\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8027 - accuracy: 0.6788\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7911 - accuracy: 0.6929\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7838 - accuracy: 0.6948\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7792 - accuracy: 0.6985\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7741 - accuracy: 0.7004\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7624 - accuracy: 0.7097\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7573 - accuracy: 0.7022\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7544 - accuracy: 0.7041\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7464 - accuracy: 0.7107\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7439 - accuracy: 0.7060\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7337 - accuracy: 0.7051\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7298 - accuracy: 0.7125\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7273 - accuracy: 0.7125\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7176 - accuracy: 0.7238\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7169 - accuracy: 0.7191\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7110 - accuracy: 0.7107\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.7024 - accuracy: 0.7172\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6992 - accuracy: 0.7238\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6967 - accuracy: 0.7172\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.7257\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6850 - accuracy: 0.7266\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6810 - accuracy: 0.7275\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6805 - accuracy: 0.7341\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6738 - accuracy: 0.7247\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6714 - accuracy: 0.7369\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6598 - accuracy: 0.7388\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6577 - accuracy: 0.7322\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6544 - accuracy: 0.7360\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6475 - accuracy: 0.7416\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6500 - accuracy: 0.7331\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.7360\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6360 - accuracy: 0.7434\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.7491\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6336 - accuracy: 0.7481\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6226 - accuracy: 0.7556\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.7481\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6165 - accuracy: 0.7434\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6131 - accuracy: 0.7500\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6144 - accuracy: 0.7556\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6062 - accuracy: 0.7500\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6011 - accuracy: 0.7575\n",
      "Test loss: 0.8272554874420166\n",
      "Test accuracy: 0.686274528503418\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "0.7983193277310925\n",
      "0.8571428571428571\n",
      "Epoch 1/20\n",
      "34/34 [==============================] - 1s 2ms/step - loss: 1.0873 - accuracy: 0.5983\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.7191 - accuracy: 0.7743\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 0s 962us/step - loss: 0.5637 - accuracy: 0.8015\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.8109\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4783 - accuracy: 0.8202\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.8202\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.8287\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4278 - accuracy: 0.8343\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8446\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8427\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8511\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8567\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8577\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8558\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3582 - accuracy: 0.8680\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8717\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8727\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8792\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8783\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.3136 - accuracy: 0.8876\n",
      "Test loss: 0.5145772695541382\n",
      "Test accuracy: 0.8151260614395142\n",
      "12/12 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.295e+03, tolerance: 2.788e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+02, tolerance: 2.098e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8683473389355743\n",
      "0.8095238095238095\n",
      "Epoch 1/15\n",
      "34/34 [==============================] - 1s 2ms/step - loss: 0.9091 - accuracy: 0.6564\n",
      "Epoch 2/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.6312 - accuracy: 0.7800\n",
      "Epoch 3/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.8062\n",
      "Epoch 4/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.8137\n",
      "Epoch 5/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.8221\n",
      "Epoch 6/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8436\n",
      "Epoch 7/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.8418\n",
      "Epoch 8/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8567\n",
      "Epoch 9/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3843 - accuracy: 0.8661\n",
      "Epoch 10/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3715 - accuracy: 0.8642\n",
      "Epoch 11/15\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8717\n",
      "Epoch 12/15\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3463 - accuracy: 0.8745\n",
      "Epoch 13/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8830\n",
      "Epoch 14/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8858\n",
      "Epoch 15/15\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8895\n",
      "Test loss: 0.5175638794898987\n",
      "Test accuracy: 0.8179271817207336\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "0.9047619047619048\n",
      "[1 4 4 1 1 1 4 1 3 1 1 1 1 1 1 1 4 4 3 4 1 1 1 1 4 4 1 4 4 4 4 1 1 1 1 1 4\n",
      " 3 1 1 1 1 4 1 1 4 1 1 4 1 1 1 1 1 4 1 1 1 4 1 4 1 1 1 1 1 4 1 4 1 1 1 4 3\n",
      " 4 1 4 4 4 3 1 1 1 4 1 4 4 4 1 1 1 1 1 1 1 1 4 1 3 1 1 1 1 1 1 4 1 1 1 4 4\n",
      " 1 4 1 4 1 1 1 1 1 1 1 1 3 1 4 4 4 3 4 4 1 1 1 4 1 4 1 1 4 3 1 4 4 4 1 1 1\n",
      " 1 1 3 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 4 1 1 1 1 1 1 4 1 3 1 4 1 1\n",
      " 1 4 4 1 4 1 3 1 3 1 1 1 1 1 4 1 1 3 4 1 1 1 1 1 1 1 1 1 3 1 4 1 1 1 1 4 4\n",
      " 1 1 1 4 4 4 1 1 1 4 4 4 1 4 1 1 1 1 1 1 4 4 4 1 1 1 4 4 4 4 4 1 1 1 3 1 1\n",
      " 1 4 4 1 3 3 1 1 1 1 1 1 1 1 1 4 1 4 1 1 4 4 1 1 1 1 1 1 4 1 1 4 1 1 4 1 4\n",
      " 1 4 1 1 1 4 3 1 1 4 1 1 4 4 1 1 1 4 1 4 1 3 1 1 4 4 1 1 4 4 1 1 3 1 1 1 1\n",
      " 4 4 4 4 1 1 4 1 1 4 4 1 1 1 1 4 1 1 4 1 3 4 1 1]\n",
      "[0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1\n",
      " 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 1 1\n",
      " 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1\n",
      " 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 0\n",
      " 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1\n",
      " 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0\n",
      " 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1\n",
      " 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.8571428571428571, 0.7983193277310925, 0.8151260614395142],\n",
       " [0.6246498599439776, 0.6974789915966386, 0.686274528503418],\n",
       " [0.8683473389355743, 0.8095238095238095, 0.8179271817207336],\n",
       " [0.9047619047619048, 0.8543417366946778, 0.8599439859390259],\n",
       " 83.4733893557423,\n",
       " 88.79551820728291]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_late_fusion_tests(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eeb1a7-39cb-4884-acdf-993ed345d82b",
   "metadata": {},
   "source": [
    "# please work my sanity is running out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f884516-a034-4173-ade8-ba96de2aa8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (14535, 110)\n",
      "Test set shape: (4846, 110)\n",
      "0.7781675608749484\n",
      "0.8345026826248453\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.7690\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5774 - accuracy: 0.7852\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5703 - accuracy: 0.7858\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5653 - accuracy: 0.7862\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5629 - accuracy: 0.7885\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5590 - accuracy: 0.7856\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5563 - accuracy: 0.7868\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5547 - accuracy: 0.7884\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5531 - accuracy: 0.7889\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5505 - accuracy: 0.7891\n",
      "Test loss: 0.5674595832824707\n",
      "Test accuracy: 0.7866281270980835\n",
      "152/152 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.715e+02, tolerance: 2.787e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8035493190260008\n",
      "0.8594717292612464\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5841 - accuracy: 0.7960\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5190 - accuracy: 0.8125\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5106 - accuracy: 0.8127\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5047 - accuracy: 0.8111\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.5001 - accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.4987 - accuracy: 0.8129\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.4974 - accuracy: 0.8137\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.4960 - accuracy: 0.8140\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.4950 - accuracy: 0.8146\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.4938 - accuracy: 0.8136\n",
      "Test loss: 0.5153268575668335\n",
      "Test accuracy: 0.808295488357544\n",
      "152/152 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.5662 - accuracy: 0.7911\n",
      "Epoch 2/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.4685 - accuracy: 0.8248\n",
      "Epoch 3/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.4315 - accuracy: 0.8347\n",
      "Epoch 4/10\n",
      "909/909 [==============================] - 1s 2ms/step - loss: 0.4075 - accuracy: 0.8454\n",
      "Epoch 5/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3888 - accuracy: 0.8544\n",
      "Epoch 6/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3693 - accuracy: 0.8592\n",
      "Epoch 7/10\n",
      "909/909 [==============================] - 1s 2ms/step - loss: 0.3528 - accuracy: 0.8643\n",
      "Epoch 8/10\n",
      "909/909 [==============================] - 1s 2ms/step - loss: 0.3364 - accuracy: 0.8718\n",
      "Epoch 9/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3187 - accuracy: 0.8797\n",
      "Epoch 10/10\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.3045 - accuracy: 0.8872\n",
      "0.8782501031778787\n",
      "0.8025175402393727\n",
      "Epoch 1/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.3396 - accuracy: 0.8818\n",
      "Epoch 2/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2615 - accuracy: 0.9031\n",
      "Epoch 3/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2584 - accuracy: 0.9035\n",
      "Epoch 4/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2559 - accuracy: 0.9060\n",
      "Epoch 5/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2536 - accuracy: 0.9051\n",
      "Epoch 6/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2518 - accuracy: 0.9069\n",
      "Epoch 7/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2511 - accuracy: 0.9075\n",
      "Epoch 8/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2503 - accuracy: 0.9061\n",
      "Epoch 9/10\n",
      "455/455 [==============================] - 1s 2ms/step - loss: 0.2494 - accuracy: 0.9068\n",
      "Epoch 10/10\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.9068\n",
      "Test loss: 0.6604052186012268\n",
      "Test accuracy: 0.8076764345169067\n",
      "152/152 [==============================] - 0s 1ms/step\n",
      "(109,)\n",
      "Epoch 1/50\n",
      "909/909 [==============================] - 3s 2ms/step - loss: 0.9809 - accuracy: 0.0211\n",
      "Epoch 2/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9761 - accuracy: 0.0239\n",
      "Epoch 3/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9759 - accuracy: 0.0250\n",
      "Epoch 4/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9758 - accuracy: 0.0246\n",
      "Epoch 5/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9758 - accuracy: 0.0257\n",
      "Epoch 6/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9758 - accuracy: 0.0254\n",
      "Epoch 7/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9757 - accuracy: 0.0252\n",
      "Epoch 8/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9757 - accuracy: 0.0260\n",
      "Epoch 9/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9757 - accuracy: 0.0261\n",
      "Epoch 10/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9756 - accuracy: 0.0257\n",
      "Epoch 11/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9756 - accuracy: 0.0259\n",
      "Epoch 12/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9755 - accuracy: 0.0254\n",
      "Epoch 13/50\n",
      "909/909 [==============================] - 2s 2ms/step - loss: 0.9756 - accuracy: 0.0250\n",
      "Epoch 14/50\n",
      "622/909 [===================>..........] - ETA: 0s - loss: 0.9771 - accuracy: 0.0256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ALL_DATA = {}\n",
    "\n",
    "for i in range(50):\n",
    "    YUH = np.random.randint(100000)\n",
    "    \n",
    "    some_inner_scoped_variable = {\"UDS\": [],\n",
    "           \"MRI\": [],\n",
    "           \"CSF\":[],\n",
    "           \"Simple Fusion\": [],\n",
    "           \"Early Fusion\": [],\n",
    "           \"Late Fusion\": []}\n",
    "    some_inner_scoped_variable[\"UDS\"] = run_uds_tests(YUH)\n",
    "    some_inner_scoped_variable[\"MRI\"] = run_mri_tests(YUH)\n",
    "    some_inner_scoped_variable[\"CSF\"] = run_csf_tests(YUH)\n",
    "    some_inner_scoped_variable[\"Simple Fusion\"] = run_simple_fusion_tests(YUH)\n",
    "    some_inner_scoped_variable[\"Late Fusion\"] = run_late_fusion_tests(YUH)\n",
    "    some_inner_scoped_variable[\"Early Fusion\"] = run_early_fusion_tests(YUH)\n",
    "    \n",
    "    ALL_DATA[YUH] = some_inner_scoped_variable\n",
    "    \n",
    "    import json\n",
    "    with open(\"my_dict.json\", \"w\") as f:\n",
    "        json.dump(ALL_DATA, f, indent=4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b576da-bc15-4b26-bc03-8aa2dfb86d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673238d7-0bb5-4ab8-981e-d674ee804dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"my_dict.json\", \"w\") as f:\n",
    "    json.dump(ALL_DATA, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32beb78c-0beb-4ad2-b84b-61840d530095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
