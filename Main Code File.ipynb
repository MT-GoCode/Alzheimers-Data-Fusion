{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4v4ir21aaba"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85701    1\n",
       "12583    1\n",
       "96670    0\n",
       "96103    1\n",
       "4948     1\n",
       "        ..\n",
       "74255    0\n",
       "39954    1\n",
       "36744    0\n",
       "85819    1\n",
       "91693    0\n",
       "Name: DECSUB.1, Length: 90465, dtype: int64"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UDS_TRAIN['DECSUB.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85701    True\n",
       "12583    True\n",
       "96670    True\n",
       "96103    True\n",
       "4948     True\n",
       "         ... \n",
       "74255    True\n",
       "39954    True\n",
       "36744    True\n",
       "85819    True\n",
       "91693    True\n",
       "Length: 90465, dtype: bool"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UDS_TRAIN['DECSUB'] == UDS_TRAIN['DECSUB.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "YK6UkhEQ27Y2"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "# Edit this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "Mnxfy7A4kd0G"
   },
   "outputs": [],
   "source": [
    "# # Import PyDrive and associated libraries.\n",
    "# # This only needs to be done once per notebook.\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# # This only needs to be done once per notebook.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "# # Download a file based on its file ID\n",
    "\n",
    "# files = {\n",
    "#     # id : name\n",
    "#     \"1DdooWeRf-z96yLWRQJgZKNQySEwK2cFm\": \"CSF_TEST\",\n",
    "#     \"1ENlaC2kmG9IM8AM7ILN3yjATVDurb0_d\": \"CSF_TRAIN\",\n",
    "#     \"1gKoMeOwwAAW5D0rW-6REPgNVilo8skQ2\": \"MRI_TEST\",\n",
    "#     \"17LNQwMOI305oStnWiXYTevjo0rGvMpnD\": \"MRI_TRAIN\",\n",
    "#     \"1WqTBJuw1uAsQdtseT8fhc6y-9C0uXsqa\": \"UDS_TEST\",\n",
    "#     \"17aHVkjjot2UehCu4weRawCCpJ5IYFIFt\": \"UDS_TRAIN\",\n",
    "#     \"1TCJLLMLMXGgf3G8RFmmt097STFVDZvKU\": \"UDS_MRI_CSF_TEST\",\n",
    "#     \"1MNpjLPC0AfZLJu4eUE0iS_3Su2eAn8JO\": \"UDS_MRI_CSF_TRAIN\",\n",
    "#     \"1PDwCyerMGYdm-QmtqNj-fyv_Jx7rEXwO\": \"UDS_MRI_TRAIN\",\n",
    "#     \"1-RgmD66M1Opxe0UvGkiMeW97QvdsMFet\": \"UDS_MRI_TEST\"\n",
    "# }\n",
    "\n",
    "# for key, value in files.items():\n",
    "#     downloaded = drive.CreateFile({'id': key})\n",
    "#     downloaded.GetContentFile(value + \".csv\")\n",
    "#     _ = pd.read_csv(value + \".csv\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "#     # PUT NACCUDSD AT THE END!\n",
    "#     columns = list(_.columns)\n",
    "#     columns.remove(\"NACCUDSD\")\n",
    "#     columns.append(\"NACCUDSD\")\n",
    "\n",
    "#     _ = _.reindex(columns=columns)\n",
    "\n",
    "#     globals()[value] = _\n",
    "\n",
    "# # This code will make bunch variable with the names above. like MRI_TEST will be a variable with MRI_TEST.csv from google drive as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "h_GP83aXBrrX"
   },
   "outputs": [],
   "source": [
    "# FOR LOCAL USE\n",
    "\n",
    "\n",
    "# FOR LOCAL USE\n",
    "names = [\"CSF_TEST\",\n",
    "\"CSF_TRAIN\",\n",
    "\"MRI_TEST\",\n",
    "\"MRI_TRAIN\",\n",
    "\"UDS_TEST\",\n",
    "\"UDS_TRAIN\",\n",
    "\"UDS_MRI_CSF_TEST\",\n",
    "\"UDS_MRI_CSF_TRAIN\",\n",
    "\"UDS_MRI_TRAIN\",\n",
    "\"UDS_MRI_TEST\"]\n",
    "\n",
    "for name in names:\n",
    "    _ = pd.read_csv(\"C:/Users/tminh/Downloads/results of yueqi code/DATA/Final 6.27 - \" + name + \".csv\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "    # PUT NACCUDSD AT THE END!\n",
    "    columns = list(_.columns)\n",
    "    columns.remove(\"NACCUDSD\")\n",
    "    columns.append(\"NACCUDSD\")\n",
    "\n",
    "    _ = _.reindex(columns=columns)\n",
    "\n",
    "    globals()[name] = _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "uUiOvUUde-uq"
   },
   "outputs": [],
   "source": [
    "def binarize(df):\n",
    "  new_df = df.copy()\n",
    "  new_df['NACCUDSD'] = new_df['NACCUDSD'].replace(1, 0).replace([2, 3, 4], 1)\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncE6BEGCkbYw"
   },
   "source": [
    "# Fusion Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvS0MhKP9oaI"
   },
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "cNiy5ja828ii"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import gc\n",
    "\n",
    "def AE(df, epochs = 10):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']  # Response variable\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0., random_state=42)\n",
    "  y_train = y_categorical\n",
    "\n",
    "  # Scale the input features\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X)\n",
    "  # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  # Create the neural network model\n",
    "  input_shape=(X_train_scaled.shape[1],)\n",
    "  print(input_shape)\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation='relu', input_shape = input_shape))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(4, activation='relu'))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(input_shape[0], activation='softmax'))  # Output layer with softmax activation for categorical variables\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train_scaled, X_train_scaled, epochs=epochs, batch_size=16)\n",
    "\n",
    "  return model, scaler\n",
    "# PART 2: GET VALUES\n",
    "\n",
    "def through_AE(df, model, scaler):\n",
    "  print(\"started\")\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # Scale the input features\n",
    "  # scaler = StandardScaler()\n",
    "  X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "  import tensorflow as tf\n",
    "  # Create a new model to get values from the last dense layer before output\n",
    "  last_dense_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-4].output)\n",
    "\n",
    "  # table = []\n",
    "  # for i in range(0, len(X_scaled)): #\n",
    "  #   # print(i)\n",
    "  #   if i % 100 == 0:\n",
    "  #     print(str(i) + \" / \" + str(len(X_scaled)))\n",
    "\n",
    "  #   input_row = X_scaled[i]  # Replace with your desired input row\n",
    "\n",
    "  #   # Obtain values from the last dense layer before output for the input row\n",
    "  #   output_values = last_dense_layer_model.predict(input_row.reshape(1, -1), verbose=0)[0]\n",
    "\n",
    "  #   table.append(output_values)\n",
    "  #   gc.collect()\n",
    "\n",
    "  # print('Output values:', output_values)\n",
    "\n",
    "  output_values = last_dense_layer_model.predict(X_scaled)\n",
    "  new_df = pd.DataFrame(output_values)\n",
    "  new_df['NACCUDSD'] = df.reset_index()['NACCUDSD']\n",
    "\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKr_8HVLfQOW"
   },
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "le97dzvMci28"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def lasso(df, al, thresh):\n",
    "\n",
    "  # Create the Lasso model\n",
    "  lasso = Lasso(alpha=al)\n",
    "\n",
    "  # Select the features\n",
    "  selector = SelectFromModel(lasso, threshold=thresh)\n",
    "  selector.fit(df.drop(\"NACCUDSD\", axis=1), df[\"NACCUDSD\"])\n",
    "\n",
    "  # Get the selected features\n",
    "  selected_features = selector.get_support()\n",
    "\n",
    "  return df.columns[:-1][selected_features].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDVacQ-7g4L5"
   },
   "source": [
    "## SDNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def through_SDNN_old(df, model, scaler):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Scale the input features\n",
    "  # scaler = StandardScaler()\n",
    "  X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "  import tensorflow as tf\n",
    "  # Create a new model to get values from the last dense layer before output\n",
    "  last_dense_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "\n",
    "    \n",
    "  pickle = []\n",
    "  for i in range(0, len(X_scaled)):\n",
    "    # print(i)\n",
    "    if i % 1000 == 0:\n",
    "      print(str(i) + \" / \" + str(len(X_scaled)))\n",
    "      that_df = pd.DataFrame(pickle)\n",
    "\n",
    "    input_row = X_scaled[i]  # Replace with your desired input row\n",
    "\n",
    "    # Obtain values from the last dense layer before output for the input row\n",
    "    output_values = last_dense_layer_model.predict(input_row.reshape(1, -1), verbose=0)[0]\n",
    "    # print(output_values)\n",
    "    pickle.append(output_values)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "  # print('Output values:', output_values)\n",
    "  # new_df = pd.DataFrame(output_values)\n",
    "  new_df = pd.DataFrame(pickle)\n",
    "  new_df['NACCUDSD'] = df.reset_index()['NACCUDSD']\n",
    "\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "__EHBhUZg5ah"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import gc\n",
    "\n",
    "def SDNN(df, epochs = 10):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']  # Response variable\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0., random_state=42)\n",
    "  y_train = y_categorical\n",
    "\n",
    "  # Scale the input features\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X)\n",
    "  # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  # Create the neural network model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(10, activation='relu'))\n",
    "  model.add(Dense(y_categorical.shape[1], activation='softmax'))  # Output layer with softmax activation for categorical variables\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=16)\n",
    "\n",
    "  return model, scaler\n",
    "# PART 2: GET VALUES\n",
    "\n",
    "def through_SDNN(df, model, scaler):\n",
    "\n",
    "  # Separate the input features (explanatory variables) and the response variable\n",
    "  X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "  y = df['NACCUDSD']\n",
    "\n",
    "  # Perform one-hot encoding on the categorical variable\n",
    "  encoder = LabelEncoder()\n",
    "  y_encoded = encoder.fit_transform(y)\n",
    "  y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "  # # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Scale the input features\n",
    "  # scaler = StandardScaler()\n",
    "  X_scaled = scaler.transform(X)\n",
    "\n",
    "    \n",
    "  import tensorflow as tf\n",
    "  # Create a new model to get values from the last dense layer before output\n",
    "  last_dense_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "  # pickle = []\n",
    "  # for i in range(0, len(X_scaled)): #\n",
    "  #   # print(i)\n",
    "  #   if i % 1000 == 0:\n",
    "  #     print(str(i) + \" / \" + str(len(X_scaled)))\n",
    "  #     that_df = pd.DataFrame(pickle)\n",
    "\n",
    "  #   input_row = X_scaled[i]  # Replace with your desired input row\n",
    "\n",
    "  #   # Obtain values from the last dense layer before output for the input row\n",
    "  #   output_values = last_dense_layer_model.predict(input_row.reshape(1, -1), verbose=0)[0]\n",
    "  #   print(output_values)\n",
    "  #   pickle.append(output_values)\n",
    "  #   gc.collect()\n",
    "\n",
    "  output_values = last_dense_layer_model.predict(X_scaled, verbose=0)\n",
    "\n",
    "  # print('Output values:', output_values)\n",
    "  new_df = pd.DataFrame(output_values)\n",
    "  # new_df = pd.DataFrame(pickle)\n",
    "  new_df['NACCUDSD'] = df.reset_index()['NACCUDSD']\n",
    "\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30byG0SHRYSZ"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "yLu1TCSwRZv2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca(train_df, test_df, var = 0.5):\n",
    "\n",
    "\n",
    "  # Load and preprocess the training dataset\n",
    "  train_data = train_df.reset_index().copy()\n",
    "  train_features = train_data.drop('NACCUDSD', axis=1)  # Exclude the response variable\n",
    "\n",
    "  # Standardize the training features by removing the mean and scaling to unit variance\n",
    "  scaler = StandardScaler()\n",
    "  scaled_train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "  # Perform PCA on the training features\n",
    "  pca = PCA()\n",
    "  pca.fit(scaled_train_features)\n",
    "\n",
    "  # Load and preprocess the test dataset\n",
    "  test_data = test_df.reset_index().copy()\n",
    "  test_features = test_data.drop('NACCUDSD', axis=1)  # Exclude the response variable\n",
    "\n",
    "  # Standardize the test features using the same scaler applied to the training features\n",
    "  scaled_test_features = scaler.transform(test_features)\n",
    "\n",
    "  # Apply the learned PCA model to both training and test features\n",
    "  train_pca = pca.transform(scaled_train_features)\n",
    "  test_pca = pca.transform(scaled_test_features)\n",
    "\n",
    "  # Construct new DataFrames for the transformed features\n",
    "  train_pca_df = pd.DataFrame(data=train_pca, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "  test_pca_df = pd.DataFrame(data=test_pca, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "\n",
    "  # Concatenate the response variable with the PCA-transformed training features\n",
    "  train_pca_df = pd.concat([train_pca_df, train_data['NACCUDSD']], axis=1)\n",
    "  test_pca_df = pd.concat([test_pca_df, test_data['NACCUDSD']], axis=1)\n",
    "\n",
    "  # Print the resulting PCA-transformed datasets\n",
    "  return [train_pca_df, test_pca_df]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM_Nm3x1Aann"
   },
   "source": [
    "# Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJG-CYRMAZEe"
   },
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "0dO-mR07Ake1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def logistic_regression(train_df, test_df, cols):\n",
    "\n",
    "  train_df = binarize(train_df)\n",
    "  test_df =  binarize(test_df)\n",
    "\n",
    "\n",
    "  X_train = train_df[cols]  # Input features # ERROR: y is included\n",
    "  y_train = train_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  X_test = test_df[cols]  # Input features # ERROR: y is included\n",
    "  y_test = test_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  logreg = LogisticRegression(solver='lbfgs')\n",
    "  logreg.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = logreg.predict(X_test)\n",
    "\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4qAIgXvfRMc"
   },
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "DqYZfJVTfSha"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def random_forest(train_df, test_df, cols):\n",
    "  # Load the dataset\n",
    "  # dataset = UDS_MRI_CSF[concat_cols+['NACCUDSD']]\n",
    "  X_train = train_df[cols]  # Input features # ERROR: y is included\n",
    "  y_train = train_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  X_test = test_df[cols]  # Input features # ERROR: y is included\n",
    "  y_test = test_df['NACCUDSD']   # Target variable\n",
    "\n",
    "  # Split the data into training and testing sets\n",
    "  # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "  rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "  # Fit the model to the training data\n",
    "  rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions on the test set\n",
    "  y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "  # Evaluate model performance\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  # print(\"Accuracy:\", accuracy)\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9k7STA-O-eX"
   },
   "source": [
    "## Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "YOovsfkRPEMd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Assuming you have train_df and test_df DataFrames\n",
    "# and the response variable is 'NACCUDSD'\n",
    "\n",
    "def neural_network(train_df, test_df, cols, epochs = 10):\n",
    "  # Separate features and target variable\n",
    "  X_train = train_df[cols]\n",
    "  y_train = train_df['NACCUDSD']-1\n",
    "  X_test = test_df[cols]\n",
    "  y_test = test_df['NACCUDSD']-1\n",
    "\n",
    "  # Convert the target variable to categorical\n",
    "  num_classes = len(set(y_train))\n",
    "  y_train_cat = to_categorical(y_train, num_classes)\n",
    "  y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "  # Standardize the features\n",
    "  scaler = StandardScaler()\n",
    "  X_train_scaled = scaler.fit_transform(X_train)\n",
    "  X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "  # Create the model\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  # Train the model\n",
    "  model.fit(X_train_scaled, y_train_cat, epochs=epochs, batch_size=32, verbose=1)\n",
    "\n",
    "  # Evaluate the model\n",
    "  loss, accuracy = model.evaluate(X_test_scaled, y_test_cat, verbose=0)\n",
    "\n",
    "  print(\"Test loss:\", loss)\n",
    "  print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaVKuAczHldn"
   },
   "source": [
    "## Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "BefKcBiUHopG"
   },
   "outputs": [],
   "source": [
    "input_mri_feats = ['NACCICV', 'NACCBRNV',\n",
    "       'NACCWMVL', 'CSFVOL', 'GRAYVOL', 'WHITEVOL', 'WMHVOL', 'HIPPOVOL',\n",
    "       'CEREALL', 'CERETISS', 'CERECSF', 'CEREGR', 'CEREWH', 'LHIPPO',\n",
    "       'RHIPPO', 'LLATVENT', 'RLATVENT', 'LATVENT', 'THIRVENT', 'LFRCORT',\n",
    "       'RFRCORT', 'FRCORT', 'LOCCORT', 'ROCCORT', 'OCCCORT', 'LPARCORT',\n",
    "       'RPARCORT', 'PARCORT', 'LTEMPCOR', 'RTEMPCOR', 'TEMPCOR', 'LCAC',\n",
    "       'LCACM', 'LCMF', 'LCMFM', 'LCUN', 'LCUNM', 'LENT', 'LENTM', 'LFUS',\n",
    "       'LFUSM', 'LINFPAR', 'LINFPARM', 'LINFTEMP', 'LINFTEMM', 'LINSULA',\n",
    "       'LINSULAM', 'LISTHC', 'LISTHCM', 'LLATOCC', 'LLATOCCM', 'LLATORBF',\n",
    "       'LLATORBM', 'LLING', 'LLINGM', 'LMEDORBF', 'LMEDORBM', 'LMIDTEMP',\n",
    "       'LMIDTEMM', 'LPARCEN', 'LPARCENM', 'LPARHIP', 'LPARHIPM',\n",
    "       'LPARSOP', 'LPARSOPM', 'LPARORB', 'LPARORBM', 'LPARTRI',\n",
    "       'LPARTRIM', 'LPERCAL', 'LPERCALM', 'LPOSCEN', 'LPOSCENM',\n",
    "       'LPOSCIN', 'LPOSCINM', 'LPRECEN', 'LPRECENM', 'LPRECUN',\n",
    "       'LPRECUNM', 'LROSANC', 'LROSANCM', 'LROSMF', 'LROSMFM', 'LSUPFR',\n",
    "       'LSUPFRM', 'LSUPPAR', 'LSUPPARM', 'LSUPTEM', 'LSUPTEMM', 'LSUPMAR',\n",
    "       'LSUPMARM', 'LTRTEM', 'LTRTEMM', 'RCAC', 'RCACM', 'RCMF', 'RCMFM',\n",
    "       'RCUN', 'RCUNM', 'RENT', 'RENTM', 'RFUS', 'RFUSM', 'RINFPAR',\n",
    "       'RINFPARM', 'RINFTEMP', 'RINFTEMM', 'RINSULA', 'RINSULAM',\n",
    "       'RISTHC', 'RISTHCM', 'RLATOCC', 'RLATOCCM', 'RLATORBF', 'RLATORBM',\n",
    "       'RLING', 'RLINGM', 'RMEDORBF', 'RMEDORBM', 'RMIDTEMP', 'RMIDTEMM',\n",
    "       'RPARCEN', 'RPARCENM', 'RPARHIP', 'RPARHIPM', 'RPARSOP',\n",
    "       'RPARSOPM', 'RPARORB', 'RPARORBM', 'RPARTRI', 'RPARTRIM',\n",
    "       'RPERCAL', 'RPERCALM', 'RPOSCEN', 'RPOSCENM', 'RPOSCIN',\n",
    "       'RPOSCINM', 'RPRECEN', 'RPRECENM', 'RPRECUN', 'RPRECUNM',\n",
    "       'RROSANC', 'RROSANCM', 'RROSMF', 'RROSMFM', 'RSUPFR', 'RSUPFRM',\n",
    "       'RSUPPAR', 'RSUPPARM', 'RSUPTEM', 'RSUPTEMM', 'RSUPMAR',\n",
    "       'RSUPMARM', 'RTRTEM', 'RTRTEMM']\n",
    "\n",
    "input_demo_feats = ['SEX', 'HISPANIC', 'HISPOR', 'RACE', 'RACEX',\n",
    "       'PRIMLANG', 'EDUC', 'MARISTAT', 'NACCLIVS', 'INDEPEND', 'RESIDENC',\n",
    "       'NACCNIHR','NACCAGE']\n",
    "input_copart_feats = ['INEDUC', 'INRELTO', 'INKNOWN', 'INLIVWTH', 'INVISITS', 'INCALLS',\n",
    "       'INRELY']\n",
    "input_fam_hist_feats = ['NACCFAM', 'NACCMOM', 'NACCDAD', 'NACCAM', 'NACCAMX',\n",
    "       'NACCAMS', 'NACCAMSX', 'NACCFM', 'NACCFMX', 'NACCFMS', 'NACCFMSX',\n",
    "       'NACCOM', 'NACCOMX', 'NACCOMS', 'NACCOMSX', 'NACCFADM', 'NACCFFTD']\n",
    "input_patient_hist_feats = ['ANYMEDS', 'TOBAC30', 'TOBAC100', 'SMOKYRS', 'PACKSPER',\n",
    "       'QUITSMOK', 'CVHATT', 'CVAFIB', 'CVANGIO', 'CVBYPASS', 'CVPACDEF',\n",
    "       'CVPACE', 'CVCHF', 'CVOTHR', 'CBSTROKE', 'NACCSTYR', 'CBTIA',\n",
    "       'NACCTIYR', 'SEIZURES', 'NACCTBI', 'TBI', 'TBIBRIEF', 'TRAUMBRF',\n",
    "       'TBIEXTEN', 'TRAUMEXT', 'TBIWOLOS', 'TRAUMCHR', 'TBIYEAR',\n",
    "       'NCOTHR', 'DIABETES', 'DIABTYPE', 'HYPERTEN', 'HYPERCHO', 'B12DEF',\n",
    "       'THYROID', 'ARTHRIT', 'ARTHTYPE', 'ARTHTYPX', 'ARTHUPEX',\n",
    "       'ARTHLOEX', 'ARTHSPIN', 'ARTHUNK', 'INCONTU', 'INCONTF', 'APNEA',\n",
    "       'RBD', 'INSOMN', 'OTHSLEEP', 'OTHSLEEX', 'ALCOHOL', 'ABUSOTHR',\n",
    "       'ABUSX', 'PTSD', 'BIPOLAR', 'SCHIZ', 'DEP2YRS', 'DEPOTHR',\n",
    "       'ANXIETY', 'OCD', 'NPSYDEV', 'PSYCDIS', 'PSYCDISX',\n",
    "       'NACCAAAS', 'NACCAANX', 'NACCAC', 'NACCACEI',\n",
    "       'NACCADEP', 'NACCAHTN', 'NACCAMD', 'NACCANGI', 'NACCAPSY',\n",
    "       'NACCBETA', 'NACCCCBS', 'NACCDBMD', 'NACCDIUR', 'NACCEMD',\n",
    "       'NACCEPMD', 'NACCHTNC', 'NACCLIPL', 'NACCNSD', 'NACCPDMD',\n",
    "       'NACCVASD']\n",
    "input_physical_feats = ['HEIGHT','WEIGHT', 'BPSYS', 'BPDIAS', 'HRATE',\n",
    "        'VISION', 'VISCORR','VISWCORR', 'HEARING', 'HEARAID', 'HEARWAID',\n",
    "        'NACCBMI']\n",
    "input_exam_feats = ['FOCLSYM','FOCLSIGN','NACCNREX', 'NORMEXAM', 'DECSUB']\n",
    "input_npi_feats = ['NPIQINF', 'NPIQINFX', 'DEL', 'DELSEV', 'HALL',\n",
    "       'HALLSEV', 'AGIT', 'AGITSEV', 'DEPD', 'DEPDSEV', 'ANX', 'ANXSEV',\n",
    "       'ELAT', 'ELATSEV', 'APA', 'APASEV', 'DISN', 'DISNSEV', 'IRR',\n",
    "       'IRRSEV', 'MOT', 'MOTSEV', 'NITE', 'NITESEV', 'APP', 'APPSEV']\n",
    "input_gds_feats = ['NOGDS', 'SATIS', 'DROPACT', 'EMPTY', 'BORED', 'SPIRITS', 'AFRAID',\n",
    "       'HAPPY', 'HELPLESS', 'STAYHOME', 'MEMPROB', 'WONDRFUL', 'WRTHLESS',\n",
    "       'ENERGY', 'HOPELESS', 'BETTER', 'NACCGDS']\n",
    "input_faq_feats = ['BILLS', 'TAXES','SHOPPING', 'GAMES', 'STOVE',\n",
    "        'MEALPREP', 'EVENTS', 'PAYATTN','REMDATES', 'TRAVEL']\n",
    "input_scd_feats = ['DECSUB']\n",
    "intput_np_feats = ['MMSEORDA',\n",
    "       'MMSEORLO', 'PENTAGON', 'NACCMMSE', 'LOGIMEM', 'MEMUNITS',\n",
    "       'MEMTIME', 'DIGIF', 'DIGIFLEN', 'DIGIB', 'DIGIBLEN', 'ANIMALS',\n",
    "       'VEG', 'TRAILA', 'TRAILARR', 'TRAILALI', 'TRAILB', 'TRAILBRR',\n",
    "       'TRAILBLI', 'BOSTON', 'MOCATOTS']\n",
    "input_gene_feats = ['NACCAPOE']\n",
    "label_feat = ['NACCUDSD']\n",
    "# cdr_feats = ['MEMORY', 'ORIENT', 'JUDGMENT', 'COMMUN', 'HOMEHOBB', 'PERSCARE', 'CDRSUM', 'CDRGLOB']\n",
    "UDS_selected_features = label_feat + input_gene_feats + intput_np_feats + input_scd_feats + input_faq_feats + input_gds_feats + input_npi_feats + input_exam_feats + input_physical_feats + input_patient_hist_feats + input_fam_hist_feats + input_copart_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOIi_V-ncnNO"
   },
   "source": [
    "# UDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS2EwYT8Xq35",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCA + three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "c2T4jqixXq36"
   },
   "outputs": [],
   "source": [
    "pca_train, pca_test = pca(train_df = UDS_TRAIN, test_df = UDS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAYX4IYQXq36",
    "outputId": "252500d9-20fb-4b02-cdc5-71b03b1cec29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu1Hxg1HXq36",
    "outputId": "1c768752-5b56-4fac-a4c3-a7a2527f8519"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tiDcGaaEXq37",
    "outputId": "d11b771e-9616-4e45-e288-7c2a7a350cc1"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-N1cB_cDgsPH",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Lasso + random forest & logistic regression & neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cK_sD0mc4d6",
    "outputId": "c831d3a0-f6df-4330-d3a1-c7529bd07622"
   },
   "outputs": [],
   "source": [
    "cols = lasso(df = UDS_TRAIN, al = 0.01, thresh = 0.04)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sZusWFdbdZA6",
    "outputId": "cc7107ee-d5fa-45a4-859f-7667638d4e20"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = UDS_TRAIN, test_df = UDS_TEST, cols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o2BqVEPE3YY",
    "outputId": "91c7f641-644e-4e93-cab2-13e11db31736"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = UDS_TRAIN, test_df = UDS_TEST, cols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey2RihfsToLV",
    "outputId": "83f56017-168a-4cca-e43a-2ba2ffd52e20"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = UDS_TRAIN, test_df = UDS_TEST, cols = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke5V1PCegvhn"
   },
   "source": [
    "## SDNN + random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sa7nwH_R7QnB",
    "outputId": "179e8fff-845e-4267-e9a4-5aecc842221d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod,scaler = SDNN(UDS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_train = through_SDNN(UDS_TRAIN, mod, scaler)\n",
    "processed_df_test = through_SDNN(UDS_TEST, mod, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE + the three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, scaler = AE(UDS_TRAIN, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_AE_train = through_AE(UDS_TRAIN, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# processed_AE_train.to_csv('uds_AE_train_proc.csv')\n",
    "processed_AE_test = through_AE(UDS_TEST, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nR1H9xglFVj6"
   },
   "source": [
    "# MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1atkI02I8iOV"
   },
   "source": [
    "### AE + three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4YBV0_y8h6o",
    "outputId": "e1182da0-30ca-476b-ed5f-2a05613eb453",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model,scaler = AE(MRI_TRAIN, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuRkhac88yDL",
    "outputId": "b4db2c90-be3e-4e48-81f8-3e5ded6371f3"
   },
   "outputs": [],
   "source": [
    "processed_AE_train = through_AE(MRI_TRAIN, model, scaler)\n",
    "processed_AE_test = through_AE(MRI_TEST, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7OlsWxI8-o_",
    "outputId": "6c09521f-f8f0-4436-acac-a69b5fa52917"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76oMBh7RFZbK",
    "outputId": "d396821c-b093-49ae-b40e-cbb33ea4574e"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8c5otAOFbyA",
    "outputId": "89a7113e-aa98-4e08-cc95-17d62116e695"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLpr29cVSZKm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PCA + three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYpJjQX_SaxE"
   },
   "outputs": [],
   "source": [
    "pca_train, pca_test = pca(train_df = MRI_TRAIN, test_df = MRI_TEST, var = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByiKRwd_TsFc",
    "outputId": "6e66746e-8e11-4a31-8acc-6095587dfe5e"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3mH2XH8UKBp",
    "outputId": "016f9099-be19-4b8b-cb0e-94443b0ef368"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NO_F8j-UNCm",
    "outputId": "74a9a563-e45c-409c-f9e4-0b47b65dc981"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDDn3v7wFVj6"
   },
   "source": [
    "### Lasso + random forest & logistic regression & neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlVu4anNFVj7",
    "outputId": "7a8ed088-653d-4862-b314-3e4defb4e343"
   },
   "outputs": [],
   "source": [
    "cols = lasso(df = MRI_TRAIN, al = 0.01, thresh = 0.05)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWAhDpUYFVj7",
    "outputId": "9d981cf8-9c12-459b-fa40-7e4f0a3dc441"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = MRI_TRAIN, test_df = MRI_TEST, cols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPNG6SozFVj7",
    "outputId": "5671d522-fe65-46e4-fbbc-04d48806a184"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = MRI_TRAIN, test_df = MRI_TEST, cols = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQikAEhHSlnd",
    "outputId": "0da6caa9-58a8-4c48-ee37-e7455582ce9d"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = MRI_TRAIN, test_df = MRI_TEST, cols = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJXFsWQ6Hu6R"
   },
   "source": [
    "### SDNN + random forest & logistic regression & neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWDg_A5PHu6R",
    "outputId": "8d1ec0eb-2a1c-4630-cc2e-83d460d13c2f"
   },
   "outputs": [],
   "source": [
    "mod, scaler = SDNN(MRI_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "DKrqMFTZHu6R",
    "outputId": "7a2c1466-66e2-4fc3-9451-e4d3f2f0d4cb"
   },
   "outputs": [],
   "source": [
    "processed_df_train = through_SDNN(MRI_TRAIN, mod, scaler)\n",
    "processed_df_test = through_SDNN(MRI_TEST, mod, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xdbw7__TIvU"
   },
   "outputs": [],
   "source": [
    "processed_df_train.columns = processed_df_train.columns.astype(str)\n",
    "processed_df_test.columns = processed_df_test.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5-WTzG7JIhu"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2YS2db9My97"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2HRpdhuSQlq"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgWg2JSrFzO1",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yh_FGjhF1qZ",
    "outputId": "9652e124-3ec2-46d1-d891-42f460346c18"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = CSF_TRAIN, test_df = CSF_TEST, cols = ['CSFABETA', 'CSFPTAU', 'CSFTTAU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfR6ZbTTGAA2",
    "outputId": "58f78f13-386c-4427-cc1d-07582c810d36"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = CSF_TRAIN, test_df = CSF_TEST, cols = ['CSFABETA', 'CSFPTAU', 'CSFTTAU'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4-unhuIHHKD"
   },
   "source": [
    "# UDS_MRI_CSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYubO2XdHhin"
   },
   "source": [
    "## No feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bzT3UPJHJLZ",
    "outputId": "75546323-53f0-46d2-c28b-ae5c04f0ad0b"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = UDS_MRI_CSF_TRAIN, test_df = UDS_MRI_CSF_TEST, cols = UDS_MRI_CSF_TRAIN.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJIjuQAsHQ95",
    "outputId": "939fffbe-7cbd-476d-ad40-6a8ec4886d8c"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = UDS_MRI_CSF_TRAIN, test_df = UDS_MRI_CSF_TEST, cols = UDS_MRI_CSF_TRAIN.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2_OXpq6UqGV",
    "outputId": "053df500-c8b2-4bb7-bf1b-619a711e27f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = UDS_MRI_CSF_TRAIN, test_df = UDS_MRI_CSF_TEST, cols = UDS_MRI_CSF_TRAIN.columns[:-1], epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vRoqtzHHnOF"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6afNQEBGpnO"
   },
   "source": [
    "### AE + three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erboyVCZGpnT",
    "outputId": "448c6b38-e069-448e-d4d9-69b77f537f2b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, scaler = AE(UDS_MRI_CSF_TRAIN, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjly1sUpGpnU",
    "outputId": "301437d9-6a20-4033-ea78-0d3e87201756",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_AE_train = through_AE(UDS_MRI_CSF_TRAIN, model, scaler)\n",
    "processed_AE_test = through_AE(UDS_MRI_CSF_TEST, model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSMQieVFGpnU",
    "outputId": "f38edb4c-af69-437f-f917-2c1254eeede3"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHoZNh4MGpnU",
    "outputId": "35c8898e-0801-431e-f0f4-d5a866b4ba2c"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hnGsdS-5GpnU",
    "outputId": "651b7e9a-6f2e-4bb8-c192-e06af8461b79"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_AE_train, test_df = processed_AE_test, cols = processed_AE_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyJ7mONxfqYq"
   },
   "source": [
    "### PCA + three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNEbAnjFfqYr"
   },
   "outputs": [],
   "source": [
    "pca_train, pca_test = pca(train_df = UDS_MRI_CSF_TRAIN, test_df = UDS_MRI_CSF_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mst_H9RgfqYs",
    "outputId": "53dd861d-3df2-4d9c-90ac-a950f12d3bb1"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYCYudaIfqYs",
    "outputId": "336007f2-39a9-451c-8497-4d4039b9f2d4"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "az5BY_IJfqYs",
    "outputId": "2abb429c-c3d2-47f4-e132-eae1f3a146be"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDGLWq6llahD"
   },
   "source": [
    "### LASSO + Logistic regression, random forest, neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijRagn6mHqJD",
    "outputId": "021f6ceb-c073-4f16-9d0d-c294f9fed413"
   },
   "outputs": [],
   "source": [
    "ze_cols = lasso(df = UDS_MRI_CSF_TRAIN, al = 0.05, thresh = 0.1)\n",
    "ze_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dE3L8j-lY2l",
    "outputId": "e05458c7-ca05-4df3-f6f1-6f893467296e"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = UDS_MRI_CSF_TRAIN, test_df = UDS_MRI_CSF_TEST, cols = ze_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dX6Kwy7mEVa",
    "outputId": "2cd8b4bb-1196-4a41-903a-d8e4a0bb860d"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = UDS_MRI_CSF_TRAIN, test_df = UDS_MRI_CSF_TEST, cols = ze_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqzih5EtmS33",
    "outputId": "27db4931-789b-4cd4-c193-995558b310a8"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = UDS_MRI_CSF_TRAIN, test_df = UDS_MRI_CSF_TEST, cols = ze_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiOhZ94WmYgx"
   },
   "source": [
    "### SDNN + Logistic regression, random forest, neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 727
    },
    "id": "YN2Z7Q0QmeqB",
    "outputId": "0da7922b-707c-44de-fbf4-cd5aa1c00c38",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod, scaler = SDNN(UDS_MRI_CSF_TRAIN, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-w3G3TzWmeqC",
    "outputId": "55cfd7a7-36f9-41e2-ac7a-0c52cbdc049a"
   },
   "outputs": [],
   "source": [
    "processed_df_train = through_SDNN(UDS_MRI_CSF_TRAIN, mod, scaler)\n",
    "processed_df_test = through_SDNN(UDS_MRI_CSF_TEST, mod, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbF_6vnvmeqC"
   },
   "outputs": [],
   "source": [
    "processed_df_train.columns = processed_df_train.columns.astype(str)\n",
    "processed_df_test.columns = processed_df_test.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ha5TaFn6meqC",
    "outputId": "62d41a06-d1f7-460d-cf6e-05287c8bf03f"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Hy5v6zjmeqC",
    "outputId": "4f18582f-c9dc-4644-bbdd-4aab55253953"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BO6huud7meqD",
    "outputId": "9bc8da5a-d654-4b6f-e6c8-6bcc40d84c16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1], epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uL3G6NcnYft"
   },
   "source": [
    "# UDS_MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39UyBY6RHQYk"
   },
   "source": [
    "## No feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QC0oB-CXHSUD",
    "outputId": "d0adcede-309c-4c35-f8e3-b0e06c289ea7"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_MRI_TRAIN.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdtiYnOrHjau",
    "outputId": "afee3676-7ce8-4422-f240-d748f0adb466"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_MRI_TRAIN.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0qGCCjCHrTy",
    "outputId": "500cac04-190a-4a39-b4dd-8fec3b6e531b"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_MRI_TRAIN.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b9byU6QY1yD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCA + three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6sc3YmzY1yD"
   },
   "outputs": [],
   "source": [
    "pca_train, pca_test = pca(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2t69lr-Y1yD",
    "outputId": "7a924c6e-23b6-48cb-e85c-6489b849f90f"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLubCALfY1yE",
    "outputId": "e090fc35-334b-4e32-d9f5-4da2a5248c5a"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZKcnhTpY1yE",
    "outputId": "b5f1ab3e-5616-4d43-89af-639ca4396dae"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = pca_train, test_df = pca_test, cols = pca_train.columns[:8], epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b24kafV-nbEY",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Lasso + Logistic regression, random forest, neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "0Na3-6Kry5kp",
    "outputId": "503d2ae2-f443-4789-8b44-0f8ee05e7683"
   },
   "outputs": [],
   "source": [
    "UDS_MRI_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obUL9z1RnZ0e",
    "outputId": "91c7b9c9-a2f1-46f1-f5fb-b125e7727f75"
   },
   "outputs": [],
   "source": [
    "them_cols = lasso(df = UDS_MRI_TRAIN, al = 0.05, thresh = 0.05) # al & thresh .05 worked best\n",
    "them_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zN76REV0ZZq",
    "outputId": "797f7bc7-f44d-4ef5-eca6-cbd55eda4b19"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = them_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-kE0F0p5e5X",
    "outputId": "23d28f85-25aa-4ca1-99e7-05dffe5ca1f1"
   },
   "outputs": [],
   "source": [
    "\n",
    "random_forest(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = lasso(df = UDS_MRI_TRAIN, al = 2 / 100, thresh = 1 / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBa94U8A3Z9y"
   },
   "outputs": [],
   "source": [
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GljryoCA0359",
    "outputId": "51c735e9-3848-4edb-f088-ceb1f080d8c3"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = them_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INPnxAzC5ywc"
   },
   "source": [
    "## SDNN + Logistic regression, random forest, neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dK1iXlS75ywd",
    "outputId": "1915d8b0-ac6e-4706-e5d2-34e6d5208299"
   },
   "outputs": [],
   "source": [
    "mod, scaler = SDNN(UDS_MRI_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlFE6V005ywd",
    "outputId": "8106e4ca-7817-497a-a8e8-91901e02431f"
   },
   "outputs": [],
   "source": [
    "processed_df_train = through_SDNN(UDS_MRI_TRAIN, mod, scaler)\n",
    "processed_df_test = through_SDNN(UDS_MRI_TEST, mod, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpXK6sGQ5ywd"
   },
   "outputs": [],
   "source": [
    "processed_df_train.columns = processed_df_train.columns.astype(str)\n",
    "processed_df_test.columns = processed_df_test.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjUoUcz55ywd",
    "outputId": "7da80d42-6376-4488-e010-f427d436f7fe"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcMvQv7A5ywd",
    "outputId": "ef2d29ec-c4c5-4a7b-8154-f7c85d94c2bf"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uwtn9l-n5ywe",
    "outputId": "8b9367e5-5426-4c9a-86b4-ffa2fe8b6e2d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_df_train, test_df = processed_df_test, epochs = 3, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE + the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod, scaler = AE(UDS_MRI_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df_train = through_AE(UDS_MRI_TRAIN, mod, scaler)\n",
    "processed_df_test = through_AE(UDS_MRI_TEST, mod, scaler)\n",
    "processed_df_train.columns = processed_df_train.columns.astype(str)\n",
    "processed_df_test.columns = processed_df_test.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network(train_df = processed_df_train, test_df = processed_df_test, cols = processed_df_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1Ao4nVVH4Gj"
   },
   "source": [
    "# UDS_MRI FLIPPED - late concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOyLtVa_Z5dS"
   },
   "outputs": [],
   "source": [
    "# This is a bit weird, but due to my data pipeline the merged UDS_MRI doesn't have the same features as UDS and MRI.\n",
    "# That was to preserve more data in each dataset. So here we are just finding columns in common.\n",
    "\n",
    "UDS_cols_in_merged = [x for x in UDS_MRI_TRAIN.columns if x not in input_mri_feats]\n",
    "UDS_cols_use = [x for x in UDS_cols_in_merged if x in UDS_TRAIN.columns]\n",
    "\n",
    "MRI_cols_in_merged = [x for x in UDS_MRI_TRAIN.columns if x not in UDS_cols_in_merged]\n",
    "MRI_cols_use = [x for x in MRI_cols_in_merged if x in MRI_TRAIN.columns] + ['NACCUDSD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gZ70Di1JL8J"
   },
   "source": [
    "## AE + Three\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V291SSywJaXV",
    "outputId": "d82c8e8d-5c39-48d9-b394-e19f9689e30e"
   },
   "outputs": [],
   "source": [
    "UDS_mod, UDS_scaler = AE(UDS_TRAIN[UDS_cols_use], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T62qls1gJaXV",
    "outputId": "0e24d5b6-585f-4e1c-d65e-6d1fc32e7119"
   },
   "outputs": [],
   "source": [
    "MRI_mod, MRI_scaler = AE(MRI_TRAIN[MRI_cols_use], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulPH-JhiJaXW",
    "outputId": "a0265be0-fac2-4959-a24a-795ceefa04c0"
   },
   "outputs": [],
   "source": [
    "processed_df_UDS_train = through_AE(UDS_MRI_TRAIN[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "processed_df_UDS_test = through_AE(UDS_MRI_TEST[UDS_cols_use], UDS_mod, UDS_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-yYAaV8GJaXW",
    "outputId": "d043df34-691b-4e5f-affc-f812477dcee4"
   },
   "outputs": [],
   "source": [
    "processed_df_MRI_train = through_AE(UDS_MRI_TRAIN[MRI_cols_use], MRI_mod, MRI_scaler)\n",
    "processed_df_MRI_test = through_AE(UDS_MRI_TEST[MRI_cols_use], MRI_mod, MRI_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "0un77Qe0JaXW",
    "outputId": "876307a8-27ae-4162-f0f9-d6df26646566"
   },
   "outputs": [],
   "source": [
    "merged_ae_train = processed_df_UDS_train[processed_df_UDS_train.columns[:-1]].merge(processed_df_MRI_train[processed_df_MRI_train.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_ae_train['NACCUDSD'] = processed_df_UDS_train['NACCUDSD']\n",
    "merged_ae_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "2SJZqsVNJaXW",
    "outputId": "eb79b79c-79c4-48ee-ec8a-ff63e7c80954"
   },
   "outputs": [],
   "source": [
    "merged_ae_test = processed_df_UDS_test[processed_df_UDS_test.columns[:-1]].merge(processed_df_MRI_test[processed_df_MRI_test.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_ae_test['NACCUDSD'] = processed_df_UDS_test['NACCUDSD']\n",
    "merged_ae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mR_l77qoJaXX",
    "outputId": "f909f4fc-3878-4896-8959-ff1264866597"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = merged_ae_train, test_df = merged_ae_test, cols = merged_ae_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3crIy-gJaXX",
    "outputId": "77c0de5c-0621-4e7c-ab6d-0a80ba1425d7"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = merged_ae_train, test_df = merged_ae_test, cols = merged_ae_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fYTMvXWyJaXX",
    "outputId": "489fc25b-cfe4-4318-8134-416da0e049fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = merged_ae_train, test_df = merged_ae_test, cols = merged_ae_train.columns[:-1], epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5Abm1SJZYif",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCA + three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz0LzcN-ZYil"
   },
   "outputs": [],
   "source": [
    "pca_UDS_train, pca_UDS_test = pca(train_df = UDS_MRI_TRAIN[UDS_cols_use], test_df = UDS_MRI_TEST[UDS_cols_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsP9KdpXaCzW"
   },
   "outputs": [],
   "source": [
    "pca_MRI_train, pca_MRI_test = pca(train_df = UDS_MRI_TRAIN[MRI_cols_use], test_df = UDS_MRI_TEST[MRI_cols_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Q9GrGZ33bNU9",
    "outputId": "bc5cbebb-cead-4d23-e812-bbfa748ae194"
   },
   "outputs": [],
   "source": [
    "merged_pca_train = pca_MRI_train[pca_MRI_train.columns[:8]].merge(pca_UDS_train[pca_UDS_train.columns[:8]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_pca_train['NACCUDSD'] = pca_MRI_train['NACCUDSD']\n",
    "merged_pca_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "eOt18Vamcbxr",
    "outputId": "6866f67a-fd86-4813-8879-8421e69209ba"
   },
   "outputs": [],
   "source": [
    "merged_pca_test = pca_MRI_test[pca_MRI_test.columns[:8]].merge(pca_UDS_test[pca_UDS_test.columns[:8]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_pca_test['NACCUDSD'] = pca_MRI_test['NACCUDSD']\n",
    "merged_pca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60ZL5OYpZYil",
    "outputId": "15e486b9-b802-4a3a-beb5-edd0dfa8fdf6"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = merged_pca_train, test_df = merged_pca_test, cols = merged_pca_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrIBmabpZYim",
    "outputId": "da81d574-4bda-490c-d226-f6a57079ae4c"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = merged_pca_train, test_df = merged_pca_test, cols = merged_pca_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y2CV13iBZYim",
    "outputId": "84dd5bc4-96f3-4fd3-92cb-0f6410003187"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = merged_pca_train, test_df = merged_pca_test, cols = merged_pca_train.columns[:-1], epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ptB3kCXILdh",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LASSO + the three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSNDFH7BH7kd",
    "outputId": "a073267e-0a27-464b-98b5-245b70d94728"
   },
   "outputs": [],
   "source": [
    "UDS_cols = lasso(df = UDS_TRAIN, al = 0.01, thresh = 0.04)\n",
    "UDS_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Op1E-pWvIE9s",
    "outputId": "20da6c60-a5ca-4e24-99e3-4b9efc522ec0"
   },
   "outputs": [],
   "source": [
    "MRI_cols = lasso(df = MRI_TRAIN, al = 0.01, thresh = 0.04)\n",
    "MRI_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1w3aiPbIUbz",
    "outputId": "18db3f80-c47e-4eb3-959d-ac3f2af2066a"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_cols+MRI_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G84tVkPiIzSu",
    "outputId": "0013645d-ad98-45a8-8351-2bb5ff4a2f3c"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  print(random_forest(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_cols+MRI_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsmc4lWoKcaH",
    "outputId": "0ac863c1-5062-4772-9e2e-73d8fa1e9268"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = UDS_MRI_TRAIN, test_df = UDS_MRI_TEST, cols = UDS_cols+MRI_cols, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBWnKGdyLO7e"
   },
   "source": [
    "## SDNN + The Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6vFxtT9LWFj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UDS_mod, UDS_scaler = SDNN(UDS_TRAIN[UDS_cols_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "607p6ydqgkSd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MRI_mod, MRI_scaler = SDNN(MRI_TRAIN[MRI_cols_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWLnMlvALWFu",
    "outputId": "979532d6-0946-4b7c-db6f-d6c3fa5eae73"
   },
   "outputs": [],
   "source": [
    "processed_df_UDS_train = through_SDNN(UDS_MRI_TRAIN[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "processed_df_UDS_test = through_SDNN(UDS_MRI_TEST[UDS_cols_use], UDS_mod, UDS_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xPcdcN8PglQ",
    "outputId": "82ef3995-132f-4b4a-95c8-3328456656f4"
   },
   "outputs": [],
   "source": [
    "processed_df_MRI_train = through_SDNN(UDS_MRI_TRAIN[MRI_cols_use], MRI_mod, MRI_scaler)\n",
    "processed_df_MRI_test = through_SDNN(UDS_MRI_TEST[MRI_cols_use], MRI_mod , MRI_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "btp_9azLhtjY",
    "outputId": "62e4e38f-a8bb-407f-a159-8f1aceaf2b65"
   },
   "outputs": [],
   "source": [
    "merged_sdnn_train = processed_df_UDS_train[processed_df_UDS_train.columns[:-1]].merge(processed_df_MRI_train[processed_df_MRI_train.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_sdnn_train['NACCUDSD'] = processed_df_UDS_train['NACCUDSD']\n",
    "merged_sdnn_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "2yLr3KC5U4-N",
    "outputId": "e8879e33-07d3-4f25-d0f1-b8cfd2ac4c5b"
   },
   "outputs": [],
   "source": [
    "merged_sdnn_test = processed_df_UDS_test[processed_df_UDS_test.columns[:-1]].merge(processed_df_MRI_test[processed_df_MRI_test.columns[:-1]], how = \"outer\", left_index=True, right_index=True)\n",
    "merged_sdnn_test['NACCUDSD'] = processed_df_UDS_test['NACCUDSD']\n",
    "merged_sdnn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sdnn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HN0atm5aVLH4",
    "outputId": "27b7638e-3de1-4dcc-f77b-ddba923f6b6d"
   },
   "outputs": [],
   "source": [
    "merged_sdnn_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQcYcxYMPmqb",
    "outputId": "95f8bd48-2b5f-4c4c-a611-8b4dcb193928"
   },
   "outputs": [],
   "source": [
    "logistic_regression(train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ref8a9VVVM91",
    "outputId": "77c0de5c-0621-4e7c-ab6d-0a80ba1425d7"
   },
   "outputs": [],
   "source": [
    "random_forest(train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWHhLeZqVPkG",
    "outputId": "7b000090-62fd-4024-8370-78ebd107d58c"
   },
   "outputs": [],
   "source": [
    "neural_network(train_df = merged_sdnn_train, test_df = merged_sdnn_test, cols = merged_sdnn_train.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UDS_cols_use[0]\n",
    "ground = merged_sdnn_test[merged_sdnn_test.columns[:-1]]\n",
    "ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000001,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 0.99999994,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0000002,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0000001,\n",
       " 0.9999999,\n",
       " 0.99999994,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9999999,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 0.99999994,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0,\n",
       " 1.0000001,\n",
       " 1.0]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_A = np.linalg.norm(A)\n",
    "    norm_B = np.linalg.norm(B)\n",
    "    return dot_product / (norm_A * norm_B)\n",
    "\n",
    "# Function to calculate cosine similarity row by row and add the result as a new column\n",
    "def calculate_cosine_similarity_row_by_row(df1, df2):\n",
    "    cosine_similarities = []\n",
    "    for index, row in df1.iterrows():\n",
    "        A = row.values\n",
    "        B = df2.loc[index].values\n",
    "        cosine_sim = cosine_similarity(A, B)\n",
    "        cosine_similarities.append(cosine_sim)\n",
    "    return cosine_similarities\n",
    "\n",
    "calculate_cosine_similarity_row_by_row(permute_merged_sdnn_test, ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_scores = []\n",
    "\n",
    "for col in UDS_cols_use + MRI_cols_use:\n",
    "    # print(col)\n",
    "    permute_UDS_MRI_TEST = UDS_MRI_TEST.copy()\n",
    "    permute_UDS_MRI_TEST[col] = np.random.permutation(permute_UDS_MRI_TEST[col])\n",
    "    \n",
    "    processed_df_UDS_test = through_SDNN(permute_UDS_MRI_TEST[UDS_cols_use], UDS_mod, UDS_scaler)\n",
    "    processed_df_MRI_test = through_SDNN(permute_UDS_MRI_TEST[MRI_cols_use], MRI_mod , MRI_scaler)\n",
    "    \n",
    "    permute_merged_sdnn_test = processed_df_UDS_test[processed_df_UDS_test.columns[:-1]].merge(processed_df_MRI_test[processed_df_MRI_test.columns[:-1]], how = \"outer\", left_index=True, \\\n",
    "                                                                                               right_index=True)\n",
    "  \n",
    "    cosine_similarities = calculate_cosine_similarity_row_by_row(permute_merged_sdnn_test, ground)\n",
    "    average_cosine_similarity = np.mean(cosine_similarities)\n",
    "    \n",
    "    feature_importance_score = 1.0 - average_cosine_similarity\n",
    "    \n",
    "    feature_importance_scores.append(feature_importance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9,  4,  3,  2, 55, 12,  1,  0, 11], dtype=int64)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_of_largest = np.argsort(feature_importance_scores)[-10:]\n",
    "indices_of_largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PAYATTN', 'EVENTS', 'TAXES', 'BILLS', 'DECSUB', 'DECSUB.1',\n",
       "       'TRAVEL', 'VEG', 'ANIMALS', 'REMDATES'], dtype='<U8')"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thing = np.array(UDS_cols_use + MRI_cols_use)\n",
    "selected_words = thing[indices_of_largest]\n",
    "selected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nap39SLg-5MG"
   },
   "source": [
    "# FUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUrWxQrWPq4x"
   },
   "source": [
    "## Concatenation + Lasso ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "k2nkLP94bKeE",
    "outputId": "9d64db1c-b364-4d88-e2dc-c7bbbec6a325"
   },
   "outputs": [],
   "source": [
    "UDS_MRI_CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKWF6ktRQo6N",
    "outputId": "c69703b9-fdc3-46c7-dbb5-429470fc8b46"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the dataframe\n",
    "df = UDS_MRI_CSF.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Create the Lasso model\n",
    "lasso = Lasso(alpha=0.05)\n",
    "\n",
    "# Select the features\n",
    "selector = SelectFromModel(lasso, threshold=0.01)\n",
    "selector.fit(df.drop(\"NACCUDSD\", axis=1), df[\"NACCUDSD\"])\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCTVi7LpQtWu",
    "outputId": "fb5872eb-bc03-4c7a-b53b-c7c3e51133f4"
   },
   "outputs": [],
   "source": [
    "# Print the selected features\n",
    "concat_cols = df.columns[:-1][selected_features].tolist()\n",
    "concat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w073cd0Q8SH"
   },
   "source": [
    "## Concatenation + SDDR ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDugvVn7RKBV",
    "outputId": "24c927e3-e93d-4849-a21a-18f35d7c25bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "df = clean_concat.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Separate the input features (explanatory variables) and the response variable\n",
    "X = df.drop('NACCUDSD', axis=1)  # Input features\n",
    "y = df['NACCUDSD']  # Response variable\n",
    "\n",
    "# Perform one-hot encoding on the categorical variable\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "y_categorical = pd.get_dummies(y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(y_categorical.shape[1], activation='softmax'))  # Output layer with softmax activation for categorical variables\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=16, validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpGvFwu_SU98",
    "outputId": "77c5874f-4741-4c36-85ae-df06d1b7d3e7"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print('Accuracy on test set:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvA8sEiBSua7",
    "outputId": "e6ce02cd-27a6-454d-914d-b229ef20065a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Create a new model to get values from the last dense layer before output\n",
    "last_dense_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "# Example input row\n",
    "input_row = X_test_scaled[0]  # Replace with your desired input row\n",
    "\n",
    "# Obtain values from the last dense layer before output for the input row\n",
    "output_values = last_dense_layer_model.predict(input_row.reshape(1, -1))\n",
    "\n",
    "print('Output values:', output_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1CXF8fbUMGP"
   },
   "outputs": [],
   "source": [
    "# successful feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZir15KbQ8w7"
   },
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNAqlKqlG3IQ"
   },
   "source": [
    "## Random Forest (from concatenation + lasso) ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maJw73EuLTpB",
    "outputId": "8a8b296e-0f41-4e09-e2c6-cd02dbceda7c"
   },
   "outputs": [],
   "source": [
    "concat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBD-IMgw3qt8",
    "outputId": "7fc9e894-e490-434d-b6e8-b4ac960fadac"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK4wTyTYG2qD",
    "outputId": "158e0752-e02a-4ba3-a734-27b281acfc03"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = UDS_MRI_CSF[concat_cols+['NACCUDSD']]\n",
    "X = UDS_MRI_CSF[['TRAILBLI', 'INRACE']]  # Input features # ERROR: y is included\n",
    "y = UDS_MRI_CSF['NACCUDSD']   # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuEI2DkiQyMX"
   },
   "source": [
    "## CNN (from lasso + concat) ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_DekncYTXfm",
    "outputId": "2d2aa67f-cf49-486c-f817-25dea2c178e4"
   },
   "outputs": [],
   "source": [
    "bad_CSF_cols = ['NACCID','CSFLPDY', 'CSFLPYR', 'CSFLPMO', 'Unnamed: 0', 'NACCADC', 'CSFPTDY', 'CSFPTYR', 'CSFPTMO','CSFLPDATE']\n",
    "clean_CSF_cols = [x for x in clean_CSF.columns.tolist() if x not in bad_CSF_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5GtSUz9Q078",
    "outputId": "79f0c535-bd13-456d-bc4c-7fd8195da5b4"
   },
   "outputs": [],
   "source": [
    "col_matrix = [UDS_cols[:9], clean_MRI_cols[:9], clean_CSF_cols[:9]+['CSFABETA']]\n",
    "col_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI3_kjgiYvDl"
   },
   "outputs": [],
   "source": [
    "data = UDS_MRI_CSF[col_matrix[0] + col_matrix[1] + col_matrix[2]].to_numpy().reshape(364, 3, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4uK6lg8RESs",
    "outputId": "906c4838-1e10-4869-cbea-fe475333e71b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the dimensions of your input data\n",
    "input_shape = (3, 9, 1)  # Assuming grayscale images with shape (3, 9)\n",
    "\n",
    "# Create a small CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Assuming you have a number of output classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have your features in a numpy array\n",
    "features = data  # Replace with your actual feature data\n",
    "labels = UDS['NACCUDSD'].to_numpy()-1# np.random.randint(num_classes, size=(num_samples))  # Replace with your actual labels\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "labels_one_hot = np.eye(4)[labels]\n",
    "\n",
    "# Train the model\n",
    "model.fit(features, labels_one_hot, batch_size=16, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVVRZ8otRONR",
    "outputId": "9083f886-1a8e-4d68-8eb2-bb9b4f7a51a4"
   },
   "outputs": [],
   "source": [
    "np.random.randint(44, size=(55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eirvj9sZR2Kr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-xgxGatDiCe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZW9AsW57Dqzm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZCmTrB-tdP8"
   },
   "source": [
    "# OBSOLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vF8WDRCCpJv"
   },
   "outputs": [],
   "source": [
    "for df in [UDS, MRI, CSF, UDS_MRI_CSF]:\n",
    "    df.mask(df == 8888.888,  np.nan,inplace=True) # MRI ROIs\n",
    "    df.mask(df == 9999.999,  np.nan,inplace=True)\n",
    "    df.mask(df == 999.9999,  np.nan,inplace=True)\n",
    "    df.mask(df == 888.8888,  np.nan,inplace=True)\n",
    "    df.mask(df == 88.8888,  np.nan,inplace=True)\n",
    "    df.mask(df == 99.9999,  np.nan,inplace=True)\n",
    "    df.mask(df == 8.8888,  np.nan,inplace=True)\n",
    "    df.mask(df == 9.9999,  np.nan,inplace=True)\n",
    "    df.replace({'EDUC': [99]},  np.nan,inplace=True)\n",
    "    df.replace({'NACCAPOE': [9]},  np.nan,inplace=True)\n",
    "    df.rename(columns={'VISITYR': 'year'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz0a-H5iApD4"
   },
   "source": [
    "\n",
    "\n",
    "#### UDS_MRI_CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "vRGG2vuoPtzd",
    "outputId": "3398fc6a-f53f-41f6-b4f4-510a1cb286a3"
   },
   "outputs": [],
   "source": [
    "UDS_MRI_CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMmcj3R0PzVd",
    "outputId": "4c6f8c00-7360-4bfc-fd13-f331d1b6a58e"
   },
   "outputs": [],
   "source": [
    "df = UDS_MRI_CSF\n",
    "\n",
    "# Count the number of NaNs for each column\n",
    "nan_counts = df.isnull().sum()\n",
    "\n",
    "bad_cols = []\n",
    "\n",
    "# Enumerate the unique counts of NaNs\n",
    "for nan_count in sorted(set(nan_counts)):\n",
    "    # Get the columns with the specified number of NaNs\n",
    "    columns = [column for column, count in nan_counts.items() if count == nan_count]\n",
    "\n",
    "    if nan_count > 40:\n",
    "      bad_cols += columns\n",
    "\n",
    "      # Print the columns\n",
    "      print(f\"Number of NaNs: {nan_count}, columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "u9USRpZpQCRP",
    "outputId": "7aa8c222-ae09-4df5-da7f-6855af879696"
   },
   "outputs": [],
   "source": [
    "clean_concat = UDS_MRI_CSF.drop(bad_cols, axis=1).dropna()\n",
    "clean_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIhdx6rtJj0y"
   },
   "source": [
    "#### more UDS cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC_z4urWFH9w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = UDS\n",
    "\n",
    "# Count the number of NaNs for each column\n",
    "nan_counts = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAcTHX45JN-O",
    "outputId": "53315638-2b52-4cd0-e722-d90f46cd352e"
   },
   "outputs": [],
   "source": [
    "bad_cols = []\n",
    "\n",
    "# Enumerate the unique counts of NaNs\n",
    "for nan_count in sorted(set(nan_counts)):\n",
    "    # Get the columns with the specified number of NaNs\n",
    "    columns = [column for column, count in nan_counts.items() if count == nan_count]\n",
    "\n",
    "    if nan_count > 20000:\n",
    "      bad_cols += columns\n",
    "\n",
    "    # Print the columns\n",
    "    print(f\"Number of NaNs: {nan_count}, columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DFpTYd_LJLK",
    "outputId": "64e3793f-971f-4140-bc6f-30474f8e6208"
   },
   "outputs": [],
   "source": [
    "len(bad_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "ZuYsm7xDJnNG",
    "outputId": "c2e4c506-0898-4ddc-9a5e-4a94542f0691"
   },
   "outputs": [],
   "source": [
    "clean_UDS = UDS.drop(bad_cols, axis=1).dropna()\n",
    "clean_UDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyFF-IyCPkXv"
   },
   "source": [
    "#### more MRI cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0puYq1NhBoOM",
    "outputId": "799b0909-677f-4bbb-add3-97dbd55fe8f5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = MRI\n",
    "\n",
    "# Count the number of NaNs for each column\n",
    "nan_counts = df.isnull().sum()\n",
    "\n",
    "bad_cols = []\n",
    "\n",
    "# Enumerate the unique counts of NaNs\n",
    "for nan_count in sorted(set(nan_counts)):\n",
    "    # Get the columns with the specified number of NaNs\n",
    "    columns = [column for column, count in nan_counts.items() if count == nan_count]\n",
    "\n",
    "    if nan_count > 5200:\n",
    "      bad_cols += columns\n",
    "\n",
    "    # Print the columns\n",
    "    print(f\"Number of NaNs: {nan_count}, columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UXXQ7SvKqTX"
   },
   "outputs": [],
   "source": [
    "bad_cols += ['MRIYR', 'MRIMO', 'MRIDY', 'MRIDATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ECs11y79BxIN",
    "outputId": "8ac116ac-11a4-48d0-da4c-005e5b5f93a1"
   },
   "outputs": [],
   "source": [
    "clean_MRI = MRI.drop(bad_cols, axis=1).dropna()\n",
    "clean_MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP432MLEOACS"
   },
   "source": [
    "### more CSF cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMNlylBkNh8G",
    "outputId": "1abf684e-1d7b-42e5-e717-dd377c823780"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = CSF\n",
    "\n",
    "# Count the number of NaNs for each column\n",
    "nan_counts = df.isnull().sum()\n",
    "\n",
    "bad_cols = []\n",
    "\n",
    "# Enumerate the unique counts of NaNs\n",
    "for nan_count in sorted(set(nan_counts)):\n",
    "    # Get the columns with the specified number of NaNs\n",
    "    columns = [column for column, count in nan_counts.items() if count == nan_count]\n",
    "\n",
    "    if nan_count > 1700:\n",
    "      bad_cols += columns\n",
    "\n",
    "    # Print the columns\n",
    "    print(f\"Number of NaNs: {nan_count}, columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upAh-2mhNmh9",
    "outputId": "aa5e05dd-5a2f-4dde-fb95-c4053c49de27"
   },
   "outputs": [],
   "source": [
    "bad_cols += ['CSFLPDY', 'CSFLPYR', 'CSFLPMO', 'Unnamed: 0', 'NACCADC', 'CSFPTDY', 'CSFPTYR', 'CSFPTMO','CSFLPDATE']\n",
    "clean_CSF = CSF.drop(bad_cols, axis=1).dropna()\n",
    "clean_CSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUdpVJLBGwfy"
   },
   "source": [
    "## attempting to fill in naccudsd for MRI and CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgwPx0jrFSdz"
   },
   "outputs": [],
   "source": [
    "mrimo = MRI['MRIMO']\n",
    "mriday = MRI['MRIDY']\n",
    "mriyr = MRI['MRIYR']\n",
    "\n",
    "# Create a new column combining the date components\n",
    "MRI['MRIDATE'] = pd.to_datetime(mriyr.astype(str) + '-' + mrimo.astype(str) + '-' + mriday.astype(str))\n",
    "visitmo = UDS['VISITMO']\n",
    "visitday = UDS['VISITDAY']\n",
    "visityr = UDS['VISITYR']\n",
    "\n",
    "# Create a new column combining the date components\n",
    "UDS['VISITDATE'] = pd.to_datetime(visityr.astype(str) + '-' + visitmo.astype(str) + '-' + visitday.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtJWjXGsOfca"
   },
   "outputs": [],
   "source": [
    "# Extract the individual columns\n",
    "# CSF LP = CSF Lumbar Puncture\n",
    "csfmo = CSF['CSFLPMO']\n",
    "csfday = CSF['CSFLPDY']\n",
    "csfyr = CSF['CSFLPYR']\n",
    "\n",
    "# Create a new column combining the date components\n",
    "CSF['CSFLPDATE'] = pd.to_datetime(csfyr.astype(str) + '-' + csfmo.astype(str) + '-' + csfday.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgmAiV47DILH"
   },
   "outputs": [],
   "source": [
    "def add_diagnosis(datf, date_name):\n",
    "  # Create an empty column in MRI to store the matched NACCUDSD values\n",
    "  datf['NACCUDSD'] = ''\n",
    "  rows_to_delete = []\n",
    "  # Iterate over each row in the MRI DataFrame\n",
    "  for index, row in datf.iterrows():\n",
    "      NACCID = row['NACCID']\n",
    "      datf_date = row[date_name]\n",
    "\n",
    "      # Filter UDS DataFrame based on NACCID and VISITDATE conditions\n",
    "      matching_rows = UDS[(UDS['NACCID'] == NACCID) & (UDS['VISITDATE'] >= datf_date - pd.DateOffset(months=18)) & (UDS['VISITDATE'] <= datf_date)]\n",
    "\n",
    "      # Check if any matching rows were found\n",
    "      if matching_rows.empty:\n",
    "          rows_to_delete.append(index)\n",
    "          # print(index)\n",
    "      else:\n",
    "          # Choose a row from the matching rows (e.g., first row)\n",
    "          chosen_row = matching_rows.iloc[0]\n",
    "          NACCUDSD = chosen_row['NACCUDSD']\n",
    "\n",
    "          # Assign the NACCUDSD value from the chosen row to the corresponding row in MRI\n",
    "          datf.at[index, 'NACCUDSD'] = NACCUDSD\n",
    "\n",
    "  datf = datf.drop(rows_to_delete)\n",
    "  # Display the updated MRI DataFrame\n",
    "  print(datf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4CBa89B2QpT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2t5cNinO-Z_",
    "outputId": "266e4e77-26ad-40ab-cf62-81860bba9c0f"
   },
   "outputs": [],
   "source": [
    "add_diagnosis(CSF, 'CSFLPDATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8zdUFr5RBxS"
   },
   "source": [
    "## More cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ai-MHhNttELG"
   },
   "outputs": [],
   "source": [
    "def deal_with_missing_values_and_add_naccudsd(data, naccudsd, thresh, remove_cols, impute, date_col):\n",
    "\n",
    "    # High thresh = remove fewer more columns\n",
    "    # Low thresh = remove more columns\n",
    "\n",
    "    df = data\n",
    "    if (naccudsd):\n",
    "      df = add_diagnosis(data)\n",
    "\n",
    "    if thresh < 1:\n",
    "      thresh = thresh * len(data)\n",
    "      # print(thresh)\n",
    "\n",
    "    # MASKING NULL VALUES\n",
    "    df.mask(df == 8888.8888,  np.nan,inplace=True)\n",
    "    df.mask(df == 8888.888,  np.nan,inplace=True) # MRI ROIs\n",
    "    df.mask(df == 9999.999,  np.nan,inplace=True)\n",
    "    df.mask(df == 999.9999,  np.nan,inplace=True)\n",
    "    df.mask(df == 888.8888,  np.nan,inplace=True)\n",
    "    df.mask(df == 88.8888,  np.nan,inplace=True)\n",
    "    df.mask(df == 99.9999,  np.nan,inplace=True)\n",
    "    df.mask(df == 8.8888,  np.nan,inplace=True)\n",
    "    df.mask(df == 9.9999,  np.nan,inplace=True)\n",
    "    df.mask(df == -4,  np.nan,inplace=True)\n",
    "    df.mask(df == -8,  np.nan,inplace=True)\n",
    "    df.replace({'EDUC': [99]},  np.nan,inplace=True)\n",
    "    df.replace({'NACCAPOE': [9]},  np.nan,inplace=True)\n",
    "    df.rename(columns={'VISITYR': 'year'},inplace=True)\n",
    "\n",
    "    # Count the number of NaNs for each column\n",
    "    nan_counts = df.isnull().sum()\n",
    "\n",
    "    bad_cols = remove_cols\n",
    "\n",
    "    # print(\"THE FOLLOWING COLUMNS ARE DROPPED\")\n",
    "\n",
    "    # Enumerate the unique counts of NaNs\n",
    "    for nan_count in sorted(set(nan_counts)):\n",
    "        # Get the columns with the specified number of NaNs\n",
    "        columns = [column for column, count in nan_counts.items() if count == nan_count]\n",
    "\n",
    "\n",
    "        if nan_count > thresh:\n",
    "          bad_cols += columns\n",
    "          # print(f\"Number of NaNs: {nan_count}, columns: {columns}\")\n",
    "\n",
    "        # Print the columns\n",
    "\n",
    "    # print('REMOVING ' + str(len(bad_cols)) + \" COLUMNS\")\n",
    "\n",
    "    # remove columns with too many missing data, irrelevant columns\n",
    "    df = df.drop(bad_cols, axis=1).dropna()\n",
    "\n",
    "    # if impute:\n",
    "    # else:\n",
    "    #   df = df.dropna()\n",
    "\n",
    "    print(\"length: \" + str(len(df)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HF0wgCrQDEWe"
   },
   "source": [
    "## Lasso + Concatenation ✅ - what about CSF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI7ZrxGkPFv3"
   },
   "source": [
    "### UDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgVuP0GJDDRH",
    "outputId": "0334e61c-c371-4bb5-e500-4f7b61f1b861"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the dataframe\n",
    "df = clean_UDS.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Create the Lasso model\n",
    "lasso = Lasso(alpha=0.001)\n",
    "\n",
    "# Select the features\n",
    "selector = SelectFromModel(lasso, threshold=0.001)\n",
    "selector.fit(df.drop(\"NACCUDSD\", axis=1), df[\"NACCUDSD\"])\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcWZqdGDMwCH",
    "outputId": "a2adc4b1-7d98-40b1-8164-c485cda2d28e"
   },
   "outputs": [],
   "source": [
    "# Print the selected features\n",
    "UDS_cols = df.columns[:-1][selected_features].tolist()\n",
    "UDS_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv0tNX5KPLIE"
   },
   "source": [
    "### MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3JO1lHePafm",
    "outputId": "c1e3293f-2084-4678-e94e-e467f48c2dbe"
   },
   "outputs": [],
   "source": [
    "clean_MRI['NACCUDSD']= clean_MRI['NACCUDSD'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bhh7ZydqPLIF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the dataframe\n",
    "df = clean_MRI.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Create the Lasso model\n",
    "lasso = Lasso(alpha=0.01)\n",
    "\n",
    "# Select the features\n",
    "selector = SelectFromModel(lasso, threshold=0.01)\n",
    "selector.fit(df.drop(\"NACCUDSD\", axis=1), df[\"NACCUDSD\"])\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HhhR8xoPLIF"
   },
   "outputs": [],
   "source": [
    "# Print the selected features\n",
    "clean_MRI_cols = df.columns[:-1][selected_features].tolist()\n",
    "clean_MRI_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7CnlPqmL-rm"
   },
   "source": [
    "### CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvwTZpxWyhAz"
   },
   "outputs": [],
   "source": [
    "concat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZxEQffOQHxp"
   },
   "outputs": [],
   "source": [
    "clean_CSF = clean_CSF[clean_CSF['NACCUDSD'] != '']\n",
    "clean_CSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8tT4cLAPwTh"
   },
   "outputs": [],
   "source": [
    "clean_CSF['NACCUDSD']= clean_CSF['NACCUDSD'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xICGzCJbN4Pl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the dataframe\n",
    "df = clean_CSF.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "# Create the Lasso model\n",
    "lasso = Lasso(alpha=0.001)\n",
    "\n",
    "# Select the features\n",
    "selector = SelectFromModel(lasso, threshold=0.001)\n",
    "selector.fit(df.drop(\"NACCUDSD\", axis=1), df[\"NACCUDSD\"])\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = selector.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imbM-cgjQg1S"
   },
   "outputs": [],
   "source": [
    "# Print the selected features\n",
    "clean_CSF_cols = df.columns[:-1][selected_features].tolist()\n",
    "clean_CSF_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 679
    },
    "id": "FuMl1EvQwZk5",
    "outputId": "4302bf2c-84d3-4dc1-ae21-30af0acc8011"
   },
   "outputs": [],
   "source": [
    "deal_with_missing_values_and_add_naccudsd(data = UDS_MRI_CSF,\n",
    "                                          naccudsd = False,\n",
    "                                          thresh = 0,\n",
    "                                          remove_cols = ['NACCVNUM'],\n",
    "                                          impute = True,\n",
    "                                          date_col = 'VISITDATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCLf43bmtwwV",
    "outputId": "f234af72-7076-4efb-95b7-359d29cb4652"
   },
   "outputs": [],
   "source": [
    "# for i in range(0,100):\n",
    "#   print(i)\n",
    "#   deal_with_missing_values_and_add_naccudsd(data = UDS_MRI_CSF,\n",
    "#                                           naccudsd = False,\n",
    "#                                           thresh = i / 100,\n",
    "#                                           remove_cols = ['NACCVNUM'],\n",
    "#                                           impute = True,\n",
    "#                                           date_col = 'VISITDATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "1sVev7RluJXX",
    "outputId": "95ffbc88-eb6f-44ae-c873-efecbf286017"
   },
   "outputs": [],
   "source": [
    "UDS_MRI_CSF"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dvS0MhKP9oaI",
    "WKr_8HVLfQOW",
    "K4qAIgXvfRMc",
    "I9k7STA-O-eX",
    "-N1cB_cDgsPH",
    "iYubO2XdHhin",
    "RIhdx6rtJj0y",
    "CyFF-IyCPkXv"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
